{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMT English to Simple English.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0tmmocWWosy/fJvYDTjG4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karumugamio/SMAIProject-Team31/blob/master/SMT_English_to_Simple_English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6UQB8jSYyGt",
        "colab_type": "code",
        "outputId": "83ef3664-4483-40f9-c240-4d1270bbafd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Opening files from Google Drive and changing current working directory to Data folder \n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "os.chdir('/gdrive/My Drive/NLAProjectWS')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJuXBH0Xpwb8",
        "colab_type": "code",
        "outputId": "df6910e6-10c4-4df7-bd8a-160d7e4018a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# if lm package import fails please update the NLTK package by !pip install nltk --upgrade to version 3.5\n",
        "!pip install nltk --upgrade"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: nltk in /usr/local/lib/python3.6/dist-packages (3.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.38.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFbWnxHCpqL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All Import Statements\n",
        "\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "\n",
        "from nltk.lm import MLE\n",
        "\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "from nltk.translate.ibm1 import IBMModel1\n",
        "from nltk.translate.api import AlignedSent\n",
        "\n",
        "import dill as pickle\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm8T8KPsZzUE",
        "colab_type": "code",
        "outputId": "f3ed6aa3-a101-48f2-cb13-2bdef28e07b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "os.chdir('/gdrive/My Drive/NLAProjectWS/workspace')\n",
        "print(\"NLTK version used in this note book is \",nltk.__version__)\n",
        "os.listdir()\n",
        "NormalEngData = '/gdrive/My Drive/NLAProjectWS/Data/v1_wiki.unsimplified'\n",
        "SimpleEngData = '/gdrive/My Drive/NLAProjectWS/Data/v1_wiki.simple'\n",
        "\n",
        "SentencePairFile = 'sentence pairs.pickle'\n",
        "IBMModelFile = 'IBMModel.pickle'\n",
        "LM_EN_File = 'en_model_trigram.pickle'\n",
        "LM_SI_File = 'si_model_trigram.pickle'\n",
        "EN_WORDS_FILE = 'all_en_unique_words.pickle'\n",
        "SI_WORDS_FILE = 'all_si_unique_words.pickle'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK version used in this note book is  3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-7oU80RaDwt",
        "colab_type": "code",
        "outputId": "03277023-732c-43a8-8a19-3d4f14cbf400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# uncomment only when you have trouble with nltk resources. This all download everything\n",
        "nltk.download('all')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTdRbZ6kqMEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allwords_en = open(NormalEngData).read()\n",
        "allwords_si = open(SimpleEngData).read()\n",
        "en_words = allwords_en.lower().split()\n",
        "en_words.append(\"NULL\")\n",
        "en_words = set (en_words)\n",
        "si_words = allwords_si.lower().split()\n",
        "si_words.append(\"NULL\")\n",
        "si_words = set (si_words)\n",
        "\n",
        "# Its important to use binary mode \n",
        "pklfile = open(EN_WORDS_FILE, 'ab') \n",
        "# source, destination \n",
        "pickle.dump(en_words, pklfile)                      \n",
        "pklfile.close() \n",
        "\n",
        "pklfile2 = open(SI_WORDS_FILE, 'ab') \n",
        "# source, destination \n",
        "pickle.dump(si_words, pklfile2)                      \n",
        "pklfile2.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "569CQnMxobPE",
        "colab_type": "code",
        "outputId": "deea9f3a-cea8-495a-e7b6-3d92d7038955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Number of Unique English Words : \",len(en_words))\n",
        "print(\"Number of Unique Simple English Words : \",len(si_words))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Unique English Words :  117830\n",
            "Number of Unique Simple English Words :  107091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVZc1XTaoqK_",
        "colab_type": "text"
      },
      "source": [
        "## Create Aligned Sentence Pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTk99lfJaQvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = []\n",
        "with open(NormalEngData) as normalLines, open(SimpleEngData) as simpleLines: \n",
        "    for x, y in zip(normalLines, simpleLines):\n",
        "        x = x.strip()\n",
        "        y = y.strip()\n",
        "        #print(\"{0}\\t{1}\".format(x, y))\n",
        "        pair = (x,y)\n",
        "        pairs.append(pair)\n",
        "totalPairs = len(pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHxMNpOZo23r",
        "colab_type": "text"
      },
      "source": [
        "## Split Entire Data Corpus into Train Test partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql2t8Xb9dJIt",
        "colab_type": "code",
        "outputId": "31c46876-8e25-4f28-edbf-c6fc78254ed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "splitRatio = 0.7\n",
        "cutoff = round(totalPairs*splitRatio)\n",
        "print(type(pairs))\n",
        "train = pairs[0:int(cutoff)]\n",
        "test = pairs[int(cutoff)+1:int(totalPairs)-1]\n",
        "print(\"Length of Total Pairs created is {}\".format(totalPairs))\n",
        "print(\"Length of train Pairs created is {}\".format(len(train)))\n",
        "print(\"Length of test Pairs created is {}\".format(len(test)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "Length of Total Pairs created is 137362\n",
            "Length of train Pairs created is 96153\n",
            "Length of test Pairs created is 41207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyLtwhUmirYP",
        "colab_type": "text"
      },
      "source": [
        "##Creating Language Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgXPiWjxpWH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_tokenized_text = []\n",
        "si_tokenized_text = []\n",
        "for p in pairs:\n",
        "    en_tokenized_text.append(nltk.word_tokenize(p[0]))\n",
        "    si_tokenized_text.append(nltk.word_tokenize(p[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sqwK2AyqNFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating placeholder for Trigram Probablity table\n",
        "trimodel = defaultdict(lambda:defaultdict(lambda:defaultdict(lambda: 0)))#1 for smoothing\n",
        "#place holder to hold next two words tuple\n",
        "tri_tuple_model = defaultdict(lambda:defaultdict(lambda: 0))#1 for smoothing\n",
        "\n",
        "# for en_sentence in train_en_file.lower().split('\\n'):\n",
        "token = nltk.word_tokenize(allwords_en)\n",
        "for w1, w2, w3 in ngrams(token,3):\n",
        "    trimodel[w1][w2][w3] += 1;\n",
        "\n",
        "for w1 in trimodel:\n",
        "    for w2 in trimodel[w1]:\n",
        "        sum_values = float(sum(trimodel[w1][w2].values()))\n",
        "        for w3 in trimodel[w1][w2]:\n",
        "            trimodel[w1][w2][w3] /= sum_values;\n",
        "            tri_tuple_model[w3][(w1,w2)] = trimodel[w1][w2][w3] #just for calculation\n",
        "en_trigram_model = tri_tuple_model\n",
        "\n",
        "\n",
        "#creating placeholder for Trigram Probablity table\n",
        "trimodel = defaultdict(lambda:defaultdict(lambda:defaultdict(lambda: 0)))#1 for smoothing\n",
        "#place holder to hold next two words tuple\n",
        "tri_tuple_model = defaultdict(lambda:defaultdict(lambda: 0))#1 for smoothing\n",
        "\n",
        "# for en_sentence in train_en_file.lower().split('\\n'):\n",
        "token = nltk.word_tokenize(allwords_si)\n",
        "for w1, w2, w3 in ngrams(token,3):\n",
        "    trimodel[w1][w2][w3] += 1;\n",
        "\n",
        "for w1 in trimodel:\n",
        "    for w2 in trimodel[w1]:\n",
        "        sum_values = float(sum(trimodel[w1][w2].values()))\n",
        "        for w3 in trimodel[w1][w2]:\n",
        "            trimodel[w1][w2][w3] /= sum_values;\n",
        "            tri_tuple_model[w3][(w1,w2)] = trimodel[w1][w2][w3] #just for calculation\n",
        "si_trigram_model = tri_tuple_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg3rplqos9fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simplifying read and preprocess for creating models\n",
        "def read_sents(filename):\n",
        "    sents = []\n",
        "    c=0\n",
        "    with open(filename,'r') as fi:\n",
        "        for li in fi:\n",
        "            sents.append(li.split())\n",
        "    return sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPbPpaidtKuZ",
        "colab_type": "code",
        "outputId": "41304fef-943d-4ecf-b157-57fdbcde86be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Arrived Max Count as 96153 by 0.7 times the available corpus.\n",
        "# We are using IBM Model 1 for creating Translation Probablity\n",
        "max_count = 96153\n",
        "eng_sents_all = read_sents(NormalEngData)\n",
        "fr_sents_all = read_sents(SimpleEngData)\n",
        "eng_sents = eng_sents_all[:max_count]\n",
        "fr_sents = fr_sents_all[:max_count]\n",
        "print(\"Size of english sentences: \", len(eng_sents))\n",
        "print(\"Size of french sentences: \", len(fr_sents))\n",
        "aligned_text = []\n",
        "for i in range(len(eng_sents)):\n",
        "    al_sent = AlignedSent(fr_sents[i],eng_sents[i])\n",
        "    aligned_text.append(al_sent)\n",
        "print(\"Training IBM model 1\")\n",
        "start = time.time()\n",
        "ibm_model = IBMModel1(aligned_text,5)\n",
        "\n",
        "end = time.time()\n",
        "executionTime = end - start\n",
        "print(\"Training complete, Time taken for training model is :\",executionTime)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of english sentences:  96153\n",
            "Size of french sentences:  96153\n",
            "Training IBM model 1\n",
            "Training complete, Time taken for training model is : 478.0055296421051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FepkGu6V6NHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Its important to use binary mode \n",
        "dbfile = open(IBMModelFile, 'ab') \n",
        "# source, destination \n",
        "pickle.dump(ibm_model, dbfile)                      \n",
        "dbfile.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iILflz-_6phW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#si_model\n",
        "# Its important to use binary mode \n",
        "dbfile1 = open(LM_SI_File, 'ab') \n",
        "# source, destination \n",
        "pickle.dump(si_trigram_model, dbfile1)                      \n",
        "dbfile1.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LmJiFxB6pq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#si_model\n",
        "# Its important to use binary mode \n",
        "dbfile2 = open(LM_EN_File, 'ab') \n",
        "# source, destination \n",
        "pickle.dump(en_trigram_model, dbfile2)                      \n",
        "dbfile2.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGivG-JS7BaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Its important to use binary mode \n",
        "dbfile3 = open(SentencePairFile, 'ab') \n",
        "# source, destination \n",
        "pickle.dump(pairs, dbfile3)                      \n",
        "dbfile3.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXVUiI1w6_ah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2cfd3911-deed-4fe1-b1c5-1e6647f91dfe"
      },
      "source": [
        "#Loading all Pickle Files for Creating actual SMT using HMM- Viterbi Algorithm\n",
        "# Source Tutorial for Viterbi- https://youtu.be/ECu_KQV3V30\n",
        "'''\n",
        "pickle_in = open(SentencePairFile,\"rb\")\n",
        "pairs = pickle.load(pickle_in)\n",
        "pickle_in = open(LM_EN_File,\"rb\")\n",
        "en_model = pickle.load(pickle_in)\n",
        "pickle_in = open(LM_SI_File,\"rb\")\n",
        "si_model = pickle.load(pickle_in)\n",
        "pickle_in = open(IBMModelFile,\"rb\")\n",
        "ibm_model = pickle.load(pickle_in)\n",
        "\n",
        "pickle_in = open(EN_WORDS_FILE,\"rb\")\n",
        "en_words = pickle.load(pickle_in)\n",
        "\n",
        "pickle_in = open(SI_WORDS_FILE,\"rb\")\n",
        "si_words = pickle.load(pickle_in)\n",
        "\n",
        "print(\"Number of Unique English Words : \",len(en_words))\n",
        "print(\"Number of Unique Simple English Words : \",len(si_words))\n",
        "'''"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npickle_in = open(SentencePairFile,\"rb\")\\npairs = pickle.load(pickle_in)\\npickle_in = open(LM_EN_File,\"rb\")\\nen_model = pickle.load(pickle_in)\\npickle_in = open(LM_SI_File,\"rb\")\\nsi_model = pickle.load(pickle_in)\\npickle_in = open(IBMModelFile,\"rb\")\\nibm_model = pickle.load(pickle_in)\\n\\npickle_in = open(EN_WORDS_FILE,\"rb\")\\nen_words = pickle.load(pickle_in)\\n\\npickle_in = open(SI_WORDS_FILE,\"rb\")\\nsi_words = pickle.load(pickle_in)\\n\\nprint(\"Number of Unique English Words : \",len(en_words))\\nprint(\"Number of Unique Simple English Words : \",len(si_words))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCDJOyr6uOk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(inputSentence):\n",
        "  inputSentence = inputSentence.split()\n",
        "\n",
        "  pi = defaultdict(lambda:float(1))\n",
        "  bp = defaultdict(lambda:\" \")\n",
        "\n",
        "  for k in range(0,len(inputSentence)):\n",
        "    for eachword in si_words:\n",
        "      for(w1,w2) in si_trigram_model[eachword] :\n",
        "        pi_now = pi[k-1]*si_trigram_model[eachword][(w1,w2)] * ibm_model.translation_table[inputSentence[k]][eachword]\n",
        "        if (pi[k]==1 or pi[k]<pi_now):\n",
        "          pi[k] = pi_now\n",
        "          bp[k+1] = eachword\n",
        "    \n",
        "    predicted_text = ''\n",
        "    for i in range (len(bp)):\n",
        "      predicted_text = predicted_text+' '+bp[i]\n",
        "  return predicted_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIMqKu0BBIam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1740ff0d-a742-4d84-d70a-58636e73fd10"
      },
      "source": [
        "EN_test = \"Soria made seven appearances for the Peru national football team during 2000\"\n",
        "translate(EN_test)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'   cat made seven appearances for the capitals national football team during 2000'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwZ4stj-JsaJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c7a4173-85eb-4e85-a10c-b6493fdf15c5"
      },
      "source": [
        "len(test)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(columns=['Source', 'Target', 'Predicted'])\n",
        "for i in range(100):\n",
        "  print(\"Translating Sentence pair #:\",i)\n",
        "  predicted_result = translate(test[i][0])\n",
        "  df.loc[i]=[test[i][0],test[i][1],predicted_result]"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translating Sentence pair #: 0\n",
            "Translating Sentence pair #: 1\n",
            "Translating Sentence pair #: 2\n",
            "Translating Sentence pair #: 3\n",
            "Translating Sentence pair #: 4\n",
            "Translating Sentence pair #: 5\n",
            "Translating Sentence pair #: 6\n",
            "Translating Sentence pair #: 7\n",
            "Translating Sentence pair #: 8\n",
            "Translating Sentence pair #: 9\n",
            "Translating Sentence pair #: 10\n",
            "Translating Sentence pair #: 11\n",
            "Translating Sentence pair #: 12\n",
            "Translating Sentence pair #: 13\n",
            "Translating Sentence pair #: 14\n",
            "Translating Sentence pair #: 15\n",
            "Translating Sentence pair #: 16\n",
            "Translating Sentence pair #: 17\n",
            "Translating Sentence pair #: 18\n",
            "Translating Sentence pair #: 19\n",
            "Translating Sentence pair #: 20\n",
            "Translating Sentence pair #: 21\n",
            "Translating Sentence pair #: 22\n",
            "Translating Sentence pair #: 23\n",
            "Translating Sentence pair #: 24\n",
            "Translating Sentence pair #: 25\n",
            "Translating Sentence pair #: 26\n",
            "Translating Sentence pair #: 27\n",
            "Translating Sentence pair #: 28\n",
            "Translating Sentence pair #: 29\n",
            "Translating Sentence pair #: 30\n",
            "Translating Sentence pair #: 31\n",
            "Translating Sentence pair #: 32\n",
            "Translating Sentence pair #: 33\n",
            "Translating Sentence pair #: 34\n",
            "Translating Sentence pair #: 35\n",
            "Translating Sentence pair #: 36\n",
            "Translating Sentence pair #: 37\n",
            "Translating Sentence pair #: 38\n",
            "Translating Sentence pair #: 39\n",
            "Translating Sentence pair #: 40\n",
            "Translating Sentence pair #: 41\n",
            "Translating Sentence pair #: 42\n",
            "Translating Sentence pair #: 43\n",
            "Translating Sentence pair #: 44\n",
            "Translating Sentence pair #: 45\n",
            "Translating Sentence pair #: 46\n",
            "Translating Sentence pair #: 47\n",
            "Translating Sentence pair #: 48\n",
            "Translating Sentence pair #: 49\n",
            "Translating Sentence pair #: 50\n",
            "Translating Sentence pair #: 51\n",
            "Translating Sentence pair #: 52\n",
            "Translating Sentence pair #: 53\n",
            "Translating Sentence pair #: 54\n",
            "Translating Sentence pair #: 55\n",
            "Translating Sentence pair #: 56\n",
            "Translating Sentence pair #: 57\n",
            "Translating Sentence pair #: 58\n",
            "Translating Sentence pair #: 59\n",
            "Translating Sentence pair #: 60\n",
            "Translating Sentence pair #: 61\n",
            "Translating Sentence pair #: 62\n",
            "Translating Sentence pair #: 63\n",
            "Translating Sentence pair #: 64\n",
            "Translating Sentence pair #: 65\n",
            "Translating Sentence pair #: 66\n",
            "Translating Sentence pair #: 67\n",
            "Translating Sentence pair #: 68\n",
            "Translating Sentence pair #: 69\n",
            "Translating Sentence pair #: 70\n",
            "Translating Sentence pair #: 71\n",
            "Translating Sentence pair #: 72\n",
            "Translating Sentence pair #: 73\n",
            "Translating Sentence pair #: 74\n",
            "Translating Sentence pair #: 75\n",
            "Translating Sentence pair #: 76\n",
            "Translating Sentence pair #: 77\n",
            "Translating Sentence pair #: 78\n",
            "Translating Sentence pair #: 79\n",
            "Translating Sentence pair #: 80\n",
            "Translating Sentence pair #: 81\n",
            "Translating Sentence pair #: 82\n",
            "Translating Sentence pair #: 83\n",
            "Translating Sentence pair #: 84\n",
            "Translating Sentence pair #: 85\n",
            "Translating Sentence pair #: 86\n",
            "Translating Sentence pair #: 87\n",
            "Translating Sentence pair #: 88\n",
            "Translating Sentence pair #: 89\n",
            "Translating Sentence pair #: 90\n",
            "Translating Sentence pair #: 91\n",
            "Translating Sentence pair #: 92\n",
            "Translating Sentence pair #: 93\n",
            "Translating Sentence pair #: 94\n",
            "Translating Sentence pair #: 95\n",
            "Translating Sentence pair #: 96\n",
            "Translating Sentence pair #: 97\n",
            "Translating Sentence pair #: 98\n",
            "Translating Sentence pair #: 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi0XAsDNXD_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06fa8832-3c35-4f50-9c37-deb5eb5e133e"
      },
      "source": [
        "pd.__version__"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExRvkJ-IYTG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_pickle('SMT 100 Sentence pd.pkl')\n",
        "df.to_csv('SMT 100 Sentence pd.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn5LF_KqbW81",
        "colab_type": "text"
      },
      "source": [
        "#end of the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC-MNUMgd95t",
        "colab_type": "text"
      },
      "source": [
        "Please refer CSv file in google Drive work space for results. \n"
      ]
    }
  ]
}