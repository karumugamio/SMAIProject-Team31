{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tanslationof_PredictedTagline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "KQgi5rlwLQoc",
        "colab_type": "code",
        "outputId": "51baab42-c2ff-4972-8b07-91f8f299de3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FDWr5w9EX-x9",
        "colab_type": "code",
        "outputId": "d596cce5-3694-452c-d285-c497ed88e92f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2wXkBJwsQUiF",
        "colab_type": "code",
        "outputId": "84c27cae-596c-4d5a-cbb0-11682adbedb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o15gRUiULQow",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Insepcting the Data"
      ]
    },
    {
      "metadata": {
        "id": "8ZSnw1r-LQoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "movie_data = pd.read_csv(\"gdrive/My Drive/movies_text.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHAvy3bWLQpe",
        "colab_type": "code",
        "outputId": "468a331b-fa47-478f-9940-97ee158162c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Remove null values and unneeded features\n",
        "movie_data.shape\n",
        "movie_data = movie_data.dropna()\n",
        "movie_data = movie_data.reset_index(drop=True)\n",
        "movie_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20404, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "2vtyxL_RgPaR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "movie_data.shape\n",
        "movie_data=movie_data.rename(columns = {\"Summary\": \"tagLine\",\"Text\":\"movie_plot\"})\n",
        "movie_data = pd.DataFrame(movie_data, columns=[\"movie_plot\",\"tagLine\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zkHn1e_BRfVj",
        "colab_type": "code",
        "outputId": "a4030da4-b1ab-43ca-c920-f80d12cf10f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Actual Data :\",movie_data.shape)\n",
        "print(\"Train data:\",movie_data.iloc[:int(movie_data.shape[0]*0.6)].shape)\n",
        "print(\"Validation Data:\",movie_data.iloc[int(movie_data.shape[0]*0.6):int(movie_data.shape[0]*0.8)].shape)\n",
        "print(\"Test Data:\",movie_data.iloc[int(movie_data.shape[0]*0.85):].shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Data : (20404, 2)\n",
            "Train data: (12242, 2)\n",
            "Validation Data: (4081, 2)\n",
            "Test Data: (3061, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z6PtsTjChq5u",
        "colab_type": "code",
        "outputId": "5845a325-247d-495f-b41f-8cb40954eb6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "movie_training_data=movie_data.iloc[:int(movie_data.shape[0]*0.6)].reset_index(drop=True)\n",
        "movie_val_data=movie_data.iloc[int(movie_data.shape[0]*0.6):int(movie_data.shape[0]*0.8)].reset_index(drop=True)\n",
        "movie_test_data=movie_data.iloc[int(movie_data.shape[0]*0.85):].reset_index(drop=True)\n",
        "   \n",
        "\n",
        "movie_training_data = movie_training_data.sample(frac=1).reset_index(drop=True)\n",
        "movie_val_data = movie_val_data.sample(frac=1).reset_index(drop=True)\n",
        "movie_test_data = movie_test_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(movie_training_data.shape)\n",
        "print(movie_val_data.shape)\n",
        "print(movie_test_data.shape)\n",
        "\n",
        "movie_data.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12242, 2)\n",
            "(4081, 2)\n",
            "(3061, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_plot</th>\n",
              "      <th>tagLine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When siblings Judy and Peter discover an encha...</td>\n",
              "      <td>Roll the dice and unleash the excitement!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A family wedding reignites the ancient feud be...</td>\n",
              "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
              "      <td>Friends are the people who let you be yourself...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Just when George Banks has recovered from his ...</td>\n",
              "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n",
              "      <td>A Los Angeles Crime Saga</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          movie_plot  \\\n",
              "0  When siblings Judy and Peter discover an encha...   \n",
              "1  A family wedding reignites the ancient feud be...   \n",
              "2  Cheated on, mistreated and stepped on, the wom...   \n",
              "3  Just when George Banks has recovered from his ...   \n",
              "4  Obsessive master thief, Neil McCauley leads a ...   \n",
              "\n",
              "                                             tagLine  \n",
              "0          Roll the dice and unleash the excitement!  \n",
              "1  Still Yelling. Still Fighting. Still Ready for...  \n",
              "2  Friends are the people who let you be yourself...  \n",
              "3  Just When His World Is Back To Normal... He's ...  \n",
              "4                           A Los Angeles Crime Saga  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "5c81RLLPLQpy",
        "colab_type": "code",
        "outputId": "40b108a4-3d90-4e23-c75d-752dac19b44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(5):\n",
        "    print(\"Plot #\",i+1)\n",
        "    print(movie_training_data.movie_plot[i])\n",
        "    print(movie_training_data.tagLine[i])\n",
        "    \n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Plot # 1\n",
            "A murderer is brought to court and only Miss Marple is unconvinced of his innocence. Once again she begins her own investigation. The third Miss Marple film starring Margaret Rutherford as the quirky amateur detective.\n",
            "New misdeeds are afoot afoot the footlights!\n",
            "\n",
            "Plot # 2\n",
            "A beautiful vampire turns a crime lord into a creature of the night.\n",
            "The movie that goes straight for the jugular.\n",
            "\n",
            "Plot # 3\n",
            "Theseus is a mortal man chosen by Zeus to lead the fight against the ruthless King Hyperion, who is on a rampage across Greece to obtain a weapon that can destroy humanity.\n",
            "The Gods need a hero.\n",
            "\n",
            "Plot # 4\n",
            "As another round of preliminary tests approach for Keitaro, so does Christmas. And as the first Christmas of the millennium, there is a rumor that if one confesses his love on this special eve it will come true. Keitaro has decided what he needs to do on this Christmas eve. But being a re-taker, can he afford taking attention away from the tests on the same day?\n",
            "If you make a wish on Christmas Eve, will it come true?\n",
            "\n",
            "Plot # 5\n",
            "Approaching forty, Ferro is unsatisfied with his life as a construction worker and part-time boxing instructor in Los Angeles, CA. After a successful bout with a young pro boxer, Ferro decides to don the gloves one last time. The movie recounts his unlikely quest for Olympic gold.\n",
            "Some Guys Don't Know Their Destiny Till It Hits Them In The Face.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IKiV-eDeLQp9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the Data"
      ]
    },
    {
      "metadata": {
        "id": "x46-twCOLQqA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Refernce for http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g1teGf2TLQqJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text, remove_stopwords = True):\n",
        "   \n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    \n",
        "   \n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    \n",
        "    \n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pP8Jq6T7S8CS",
        "colab_type": "code",
        "outputId": "ed012d5d-b94e-4f8f-aa5f-0e9706f8721d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#nltk.download()\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "0Dyv4lxIZbcD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqWB7N4cLQqV",
        "colab_type": "code",
        "outputId": "16d619c1-ef93-40d5-af0a-2889143dbd5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# Clean the Training Taglines and Movie Plots \n",
        "clean_training_tag = []\n",
        "for tagline_train in movie_training_data.tagLine:\n",
        "    clean_training_tag.append(clean_text(tagline_train, remove_stopwords=False))\n",
        "print(\"Training Taglines are complete.\")\n",
        "\n",
        "\n",
        "\n",
        "clean_training_plot = []\n",
        "for movieplot_train in movie_training_data.movie_plot:\n",
        "    clean_training_plot.append(clean_text(movieplot_train))\n",
        "print(\"Training movie plots are complete.\")\n",
        "\n",
        "# Clean the Validation Taglines and Movie Plots \n",
        "clean_val_tag = []\n",
        "for tagline_val in movie_val_data.tagLine:\n",
        "    clean_val_tag.append(clean_text(tagline_val, remove_stopwords=False))\n",
        "print(\"Validation Taglines are complete.\")\n",
        "\n",
        "clean_val_plot = []\n",
        "for movieplot_val in movie_val_data.movie_plot:\n",
        "    clean_val_plot.append(clean_text(movieplot_val))\n",
        "print(\"Validation movie plots are complete.\")\n",
        "\n",
        "\n",
        "# Clean the Testing Taglines and Movie Plots \n",
        "clean_test_tag = []\n",
        "for tagline_test in movie_test_data.tagLine:\n",
        "    clean_test_tag.append(clean_text(tagline_test, remove_stopwords=False))\n",
        "print(\"Testing Taglines are complete.\")\n",
        "\n",
        "clean_test_plot = []\n",
        "for movieplot_test in movie_test_data.movie_plot:\n",
        "    clean_test_plot.append(clean_text(movieplot_test))\n",
        "print(\"Testing movie plots are complete.\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Taglines are complete.\n",
            "Training movie plots are complete.\n",
            "Validation Taglines are complete.\n",
            "Validation movie plots are complete.\n",
            "Testing Taglines are complete.\n",
            "Testing movie plots are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EghwEfRKLQqg",
        "colab_type": "code",
        "outputId": "9dbaee3c-8930-4481-cc75-34641f83b6e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
        "for i in range(5):\n",
        "    print(\"Clean Plot #\",i+1)\n",
        "    print(clean_training_plot[i])\n",
        "    print(clean_training_tag[i])\n",
        "    \n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clean Plot # 1\n",
            "murderer brought court miss marple unconvinced innocence begins investigation third miss marple film starring margaret rutherford quirky amateur detective\n",
            "new misdeeds are afoot afoot the footlights \n",
            "\n",
            "Clean Plot # 2\n",
            "beautiful vampire turns crime lord creature night\n",
            "the movie that goes straight for the jugular \n",
            "\n",
            "Clean Plot # 3\n",
            "theseus mortal man chosen zeus lead fight ruthless king hyperion rampage across greece obtain weapon destroy humanity\n",
            "the gods need a hero \n",
            "\n",
            "Clean Plot # 4\n",
            "another round preliminary tests approach keitaro christmas first christmas millennium rumor one confesses love special eve come true keitaro decided needs christmas eve taker afford taking attention away tests day\n",
            "if you make a wish on christmas eve  will it come true \n",
            "\n",
            "Clean Plot # 5\n",
            "approaching forty ferro unsatisfied life construction worker part time boxing instructor los angeles ca successful bout young pro boxer ferro decides gloves one last time movie recounts unlikely quest olympic gold\n",
            "some guys do not know their destiny till it hits them in the face \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X8rS8qoCLQqv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_words(count_dict, text):\n",
        "    \n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Byd_z-CLQq5",
        "colab_type": "code",
        "outputId": "6491c288-373b-4bfe-e7cf-95b0b12aea0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "word_counts = {}\n",
        "\n",
        "count_words(word_counts, clean_training_tag)\n",
        "count_words(word_counts, clean_training_plot)\n",
        "            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 37183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SRQTXw2-LQrH",
        "colab_type": "code",
        "outputId": "4fa6ed85-6561-4dac-ef5c-69cc509f3e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "embeddings_index = {}\n",
        "with open('gdrive/My Drive/numberbatch-en.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 417195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NmTzhHv8LQrT",
        "colab_type": "code",
        "outputId": "c4b5c825-27c2-4e2c-89ce-ceeb611debe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "missing_words = 0\n",
        "threshold = 2\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round((1.0*missing_words/len(word_counts))*100,4)\n",
        "\n",
        "print(\"Number of words missing from CN:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words missing from CN: 464\n",
            "Percent of words that are missing from vocabulary: 1.2479%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZzurM03VlzJa",
        "colab_type": "code",
        "outputId": "486df05b-e2ed-4b7f-f70e-b38aa4d1039b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 37183\n",
            "Number of words we will use: 33113\n",
            "Percent of words we will use: 89.05%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d_c9mCQtLQrx",
        "colab_type": "code",
        "outputId": "127b6ff0-7254-4976-fca7-051fe32e2f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        \n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d_pENhlVLQr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    '''Convert words in text to an integer.\n",
        "       If word is not in vocab_to_int, use UNK's integer.\n",
        "       Total the number of words and UNKs.\n",
        "       Add EOS token to the end of texts'''\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R6UGbTa7LQsE",
        "colab_type": "code",
        "outputId": "a61d8ae1-74fa-4e78-ddf3-92adfb83e0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_training_tag, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_training_plot, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in Tagline:\", word_count)\n",
        "print(\"Total number of UNKs in Tagline:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in Tagline: 495597\n",
            "Total number of UNKs in Tagline: 4074\n",
            "Percent of words that are UNK: 0.8200000000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p9JQy1ZILQsN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_lengths(text):\n",
        "    '\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NtMSBO3WLQsV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "#print(\"Summaries:\")\n",
        "#print(lengths_summaries.describe())\n",
        "#print()\n",
        "#print(\"Texts:\")\n",
        "#print(lengths_texts.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f7a5lIyOLQs6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unk_counter(sentence):\n",
        "    \n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k1lki7N1LQtE",
        "colab_type": "code",
        "outputId": "36ab627e-e5c9-451e-917b-24372430b235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 200\n",
        "max_summary_length = 50\n",
        "min_length = 2\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in range(min(lengths_texts.counts), max_text_length): \n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "        \n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))\n",
        "print(type(sorted_summaries))\n",
        "print(type(sorted_texts))\n",
        "print(sorted_summaries[:10])\n",
        "print(sorted_texts[:10])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11205\n",
            "11205\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "[[83, 1440, 146, 2967, 2968], [69, 486, 69], [211, 1528, 14, 1089, 10, 32, 83, 125, 348], [216, 56, 1058, 154, 644], [1351, 1352, 4, 102, 4, 207, 573], [4, 6512, 94, 4, 6512, 3593, 4, 6512, 347], [4, 80, 56, 4, 135, 56, 2556], [24, 77, 4, 1840, 56, 424, 1824, 125, 1294, 64, 2630, 2631], [362, 861, 948, 853, 4, 2940, 15, 37, 179, 59, 572, 362, 5569, 5570, 37, 1784, 594], [4, 393, 5803, 583, 130, 4, 393, 5804, 56, 574, 86]]\n",
            "[[16036, 134, 33111], [16036, 134, 33111], [123, 83, 1829, 33111], [1058, 154, 644, 33111], [2897, 9354, 473, 33111], [514, 6512, 2149, 33111], [80, 135, 2556, 33111], [167, 181, 7320, 15065, 33111], [473, 1873, 4327, 10669, 33111], [8040, 3761, 2398, 187, 33111]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W6DDETeALQtM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the Model"
      ]
    },
    {
      "metadata": {
        "id": "c6g5-4SeLQtO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_inputs():\n",
        "    \n",
        "    \n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7EhYpPaNLQtZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XKyj-X_LQtn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    \n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "   \n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7_0YbtcLQt_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    \n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KVkYqZitkMTB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pq5x5WfkLQuG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpnViGejLQuN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "            \n",
        "    \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Ht7T0DtLQuT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "   \n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2oIs-ANVLQuZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HTVBMCLqLQuf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vNMW6ONdLQun",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "epochs = 30\n",
        "batch_size = 64\n",
        "rnn_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.002\n",
        "keep_probability = 0.75"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srcAwEmPkjEx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9FNu1oJeLQuu",
        "colab_type": "code",
        "outputId": "01ec1224-cf16-4bae-d823-685acb31dbcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "\n",
        "with train_graph.as_default():\n",
        "    \n",
        "     \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    \n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "   \n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    \n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        \n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        \n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        \n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph is built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KVv3Zz19LQu6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ]
    },
    {
      "metadata": {
        "id": "mUYUT0pyLQvH",
        "colab_type": "code",
        "outputId": "0f3e670c-0b53-4c7d-b415-524042a73608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "start =0\n",
        "end = start + 5000\n",
        "sorted_summaries_short = sorted_summaries[start:end]\n",
        "sorted_texts_short = sorted_texts[start:end]\n",
        "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shortest text length: 3\n",
            "The longest text length: 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "lfR2LOmXLQvQ",
        "colab_type": "code",
        "outputId": "8a434628-928a-43b5-f3f2-1fb6396835f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4607
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "learning_rate_decay = 0.95\n",
        "min_learning_rate = 0.0005\n",
        "display_step = 20 \n",
        "stop_early = 0 \n",
        "stop = 3 \n",
        "per_epoch = 3 \n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] \n",
        "\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    \n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts_short) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "\n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "                \n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        \n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   1/30 Batch   20/78 - Loss:  3.493, Seconds: 3.42\n",
            "Average loss for this update: 3.394\n",
            "New Record!\n",
            "Epoch   1/30 Batch   40/78 - Loss:  2.676, Seconds: 3.45\n",
            "Average loss for this update: 2.441\n",
            "New Record!\n",
            "Epoch   1/30 Batch   60/78 - Loss:  2.354, Seconds: 3.13\n",
            "Average loss for this update: 2.454\n",
            "No Improvement.\n",
            "Epoch   2/30 Batch   20/78 - Loss:  2.255, Seconds: 2.52\n",
            "Average loss for this update: 2.353\n",
            "New Record!\n",
            "Epoch   2/30 Batch   40/78 - Loss:  2.486, Seconds: 2.38\n",
            "Average loss for this update: 2.291\n",
            "New Record!\n",
            "Epoch   2/30 Batch   60/78 - Loss:  2.234, Seconds: 2.57\n",
            "Average loss for this update: 2.337\n",
            "No Improvement.\n",
            "Epoch   3/30 Batch   20/78 - Loss:  2.210, Seconds: 2.61\n",
            "Average loss for this update: 2.303\n",
            "No Improvement.\n",
            "Epoch   3/30 Batch   40/78 - Loss:  2.412, Seconds: 2.43\n",
            "Average loss for this update: 2.217\n",
            "New Record!\n",
            "Epoch   3/30 Batch   60/78 - Loss:  2.159, Seconds: 2.62\n",
            "Average loss for this update: 2.257\n",
            "No Improvement.\n",
            "Epoch   4/30 Batch   20/78 - Loss:  2.143, Seconds: 2.55\n",
            "Average loss for this update: 2.229\n",
            "No Improvement.\n",
            "Epoch   4/30 Batch   40/78 - Loss:  2.316, Seconds: 2.59\n",
            "Average loss for this update: 2.129\n",
            "New Record!\n",
            "Epoch   4/30 Batch   60/78 - Loss:  2.070, Seconds: 2.52\n",
            "Average loss for this update: 2.157\n",
            "No Improvement.\n",
            "Epoch   5/30 Batch   20/78 - Loss:  2.076, Seconds: 2.54\n",
            "Average loss for this update: 2.156\n",
            "No Improvement.\n",
            "Epoch   5/30 Batch   40/78 - Loss:  2.216, Seconds: 2.41\n",
            "Average loss for this update: 2.034\n",
            "New Record!\n",
            "Epoch   5/30 Batch   60/78 - Loss:  1.981, Seconds: 2.47\n",
            "Average loss for this update: 2.079\n",
            "No Improvement.\n",
            "Epoch   6/30 Batch   20/78 - Loss:  2.012, Seconds: 2.68\n",
            "Average loss for this update: 2.086\n",
            "No Improvement.\n",
            "Epoch   6/30 Batch   40/78 - Loss:  2.141, Seconds: 2.57\n",
            "Average loss for this update: 1.978\n",
            "New Record!\n",
            "Epoch   6/30 Batch   60/78 - Loss:  1.939, Seconds: 2.66\n",
            "Average loss for this update: 2.028\n",
            "No Improvement.\n",
            "Epoch   7/30 Batch   20/78 - Loss:  1.967, Seconds: 2.52\n",
            "Average loss for this update: 2.037\n",
            "No Improvement.\n",
            "Epoch   7/30 Batch   40/78 - Loss:  2.091, Seconds: 2.42\n",
            "Average loss for this update: 1.923\n",
            "New Record!\n",
            "Epoch   7/30 Batch   60/78 - Loss:  1.875, Seconds: 2.50\n",
            "Average loss for this update: 1.971\n",
            "No Improvement.\n",
            "Epoch   8/30 Batch   20/78 - Loss:  1.916, Seconds: 2.64\n",
            "Average loss for this update: 1.986\n",
            "No Improvement.\n",
            "Epoch   8/30 Batch   40/78 - Loss:  2.034, Seconds: 2.52\n",
            "Average loss for this update: 1.868\n",
            "New Record!\n",
            "Epoch   8/30 Batch   60/78 - Loss:  1.816, Seconds: 2.51\n",
            "Average loss for this update: 1.907\n",
            "No Improvement.\n",
            "Epoch   9/30 Batch   20/78 - Loss:  1.873, Seconds: 2.56\n",
            "Average loss for this update: 1.94\n",
            "No Improvement.\n",
            "Epoch   9/30 Batch   40/78 - Loss:  1.977, Seconds: 2.44\n",
            "Average loss for this update: 1.808\n",
            "New Record!\n",
            "Epoch   9/30 Batch   60/78 - Loss:  1.763, Seconds: 2.53\n",
            "Average loss for this update: 1.858\n",
            "No Improvement.\n",
            "Epoch  10/30 Batch   20/78 - Loss:  1.832, Seconds: 2.55\n",
            "Average loss for this update: 1.896\n",
            "No Improvement.\n",
            "Epoch  10/30 Batch   40/78 - Loss:  1.920, Seconds: 2.44\n",
            "Average loss for this update: 1.755\n",
            "New Record!\n",
            "Epoch  10/30 Batch   60/78 - Loss:  1.709, Seconds: 2.49\n",
            "Average loss for this update: 1.799\n",
            "No Improvement.\n",
            "Epoch  11/30 Batch   20/78 - Loss:  1.775, Seconds: 2.54\n",
            "Average loss for this update: 1.838\n",
            "No Improvement.\n",
            "Epoch  11/30 Batch   40/78 - Loss:  1.857, Seconds: 2.45\n",
            "Average loss for this update: 1.695\n",
            "New Record!\n",
            "Epoch  11/30 Batch   60/78 - Loss:  1.657, Seconds: 2.51\n",
            "Average loss for this update: 1.744\n",
            "No Improvement.\n",
            "Epoch  12/30 Batch   20/78 - Loss:  1.734, Seconds: 2.51\n",
            "Average loss for this update: 1.792\n",
            "No Improvement.\n",
            "Epoch  12/30 Batch   40/78 - Loss:  1.803, Seconds: 2.46\n",
            "Average loss for this update: 1.649\n",
            "New Record!\n",
            "Epoch  12/30 Batch   60/78 - Loss:  1.608, Seconds: 2.50\n",
            "Average loss for this update: 1.695\n",
            "No Improvement.\n",
            "Epoch  13/30 Batch   20/78 - Loss:  1.679, Seconds: 2.56\n",
            "Average loss for this update: 1.739\n",
            "No Improvement.\n",
            "Epoch  13/30 Batch   40/78 - Loss:  1.742, Seconds: 2.44\n",
            "Average loss for this update: 1.585\n",
            "New Record!\n",
            "Epoch  13/30 Batch   60/78 - Loss:  1.548, Seconds: 2.58\n",
            "Average loss for this update: 1.632\n",
            "No Improvement.\n",
            "Epoch  14/30 Batch   20/78 - Loss:  1.638, Seconds: 2.83\n",
            "Average loss for this update: 1.691\n",
            "No Improvement.\n",
            "Epoch  14/30 Batch   40/78 - Loss:  1.684, Seconds: 2.41\n",
            "Average loss for this update: 1.528\n",
            "New Record!\n",
            "Epoch  14/30 Batch   60/78 - Loss:  1.476, Seconds: 2.43\n",
            "Average loss for this update: 1.558\n",
            "No Improvement.\n",
            "Epoch  15/30 Batch   20/78 - Loss:  1.596, Seconds: 2.54\n",
            "Average loss for this update: 1.639\n",
            "No Improvement.\n",
            "Epoch  15/30 Batch   40/78 - Loss:  1.613, Seconds: 2.42\n",
            "Average loss for this update: 1.474\n",
            "New Record!\n",
            "Epoch  15/30 Batch   60/78 - Loss:  1.441, Seconds: 2.47\n",
            "Average loss for this update: 1.513\n",
            "No Improvement.\n",
            "Epoch  16/30 Batch   20/78 - Loss:  1.539, Seconds: 2.55\n",
            "Average loss for this update: 1.58\n",
            "No Improvement.\n",
            "Epoch  16/30 Batch   40/78 - Loss:  1.559, Seconds: 2.65\n",
            "Average loss for this update: 1.422\n",
            "New Record!\n",
            "Epoch  16/30 Batch   60/78 - Loss:  1.384, Seconds: 2.47\n",
            "Average loss for this update: 1.459\n",
            "No Improvement.\n",
            "Epoch  17/30 Batch   20/78 - Loss:  1.478, Seconds: 2.61\n",
            "Average loss for this update: 1.515\n",
            "No Improvement.\n",
            "Epoch  17/30 Batch   40/78 - Loss:  1.496, Seconds: 2.62\n",
            "Average loss for this update: 1.362\n",
            "New Record!\n",
            "Epoch  17/30 Batch   60/78 - Loss:  1.319, Seconds: 2.53\n",
            "Average loss for this update: 1.393\n",
            "No Improvement.\n",
            "Epoch  18/30 Batch   20/78 - Loss:  1.426, Seconds: 2.58\n",
            "Average loss for this update: 1.458\n",
            "No Improvement.\n",
            "Epoch  18/30 Batch   40/78 - Loss:  1.426, Seconds: 2.43\n",
            "Average loss for this update: 1.303\n",
            "New Record!\n",
            "Epoch  18/30 Batch   60/78 - Loss:  1.257, Seconds: 2.52\n",
            "Average loss for this update: 1.321\n",
            "No Improvement.\n",
            "Epoch  19/30 Batch   20/78 - Loss:  1.413, Seconds: 2.67\n",
            "Average loss for this update: 1.434\n",
            "No Improvement.\n",
            "Epoch  19/30 Batch   40/78 - Loss:  1.367, Seconds: 2.53\n",
            "Average loss for this update: 1.253\n",
            "New Record!\n",
            "Epoch  19/30 Batch   60/78 - Loss:  1.210, Seconds: 2.48\n",
            "Average loss for this update: 1.26\n",
            "No Improvement.\n",
            "Epoch  20/30 Batch   20/78 - Loss:  1.336, Seconds: 2.58\n",
            "Average loss for this update: 1.358\n",
            "No Improvement.\n",
            "Epoch  20/30 Batch   40/78 - Loss:  1.301, Seconds: 2.43\n",
            "Average loss for this update: 1.191\n",
            "New Record!\n",
            "Epoch  20/30 Batch   60/78 - Loss:  1.155, Seconds: 2.65\n",
            "Average loss for this update: 1.207\n",
            "No Improvement.\n",
            "Epoch  21/30 Batch   20/78 - Loss:  1.294, Seconds: 2.93\n",
            "Average loss for this update: 1.313\n",
            "No Improvement.\n",
            "Epoch  21/30 Batch   40/78 - Loss:  1.245, Seconds: 2.38\n",
            "Average loss for this update: 1.138\n",
            "New Record!\n",
            "Epoch  21/30 Batch   60/78 - Loss:  1.104, Seconds: 2.49\n",
            "Average loss for this update: 1.156\n",
            "No Improvement.\n",
            "Epoch  22/30 Batch   20/78 - Loss:  1.242, Seconds: 2.56\n",
            "Average loss for this update: 1.255\n",
            "No Improvement.\n",
            "Epoch  22/30 Batch   40/78 - Loss:  1.190, Seconds: 2.42\n",
            "Average loss for this update: 1.104\n",
            "New Record!\n",
            "Epoch  22/30 Batch   60/78 - Loss:  1.074, Seconds: 2.49\n",
            "Average loss for this update: 1.121\n",
            "No Improvement.\n",
            "Epoch  23/30 Batch   20/78 - Loss:  1.198, Seconds: 2.55\n",
            "Average loss for this update: 1.208\n",
            "No Improvement.\n",
            "Epoch  23/30 Batch   40/78 - Loss:  1.141, Seconds: 2.47\n",
            "Average loss for this update: 1.055\n",
            "New Record!\n",
            "Epoch  23/30 Batch   60/78 - Loss:  1.014, Seconds: 2.49\n",
            "Average loss for this update: 1.057\n",
            "No Improvement.\n",
            "Epoch  24/30 Batch   20/78 - Loss:  1.160, Seconds: 2.54\n",
            "Average loss for this update: 1.17\n",
            "No Improvement.\n",
            "Epoch  24/30 Batch   40/78 - Loss:  1.094, Seconds: 2.58\n",
            "Average loss for this update: 1.006\n",
            "New Record!\n",
            "Epoch  24/30 Batch   60/78 - Loss:  0.970, Seconds: 2.61\n",
            "Average loss for this update: 1.009\n",
            "No Improvement.\n",
            "Epoch  25/30 Batch   20/78 - Loss:  1.114, Seconds: 2.49\n",
            "Average loss for this update: 1.119\n",
            "No Improvement.\n",
            "Epoch  25/30 Batch   40/78 - Loss:  1.038, Seconds: 2.49\n",
            "Average loss for this update: 0.957\n",
            "New Record!\n",
            "Epoch  25/30 Batch   60/78 - Loss:  0.929, Seconds: 2.49\n",
            "Average loss for this update: 0.968\n",
            "No Improvement.\n",
            "Epoch  26/30 Batch   20/78 - Loss:  1.071, Seconds: 2.64\n",
            "Average loss for this update: 1.074\n",
            "No Improvement.\n",
            "Epoch  26/30 Batch   40/78 - Loss:  0.990, Seconds: 2.44\n",
            "Average loss for this update: 0.913\n",
            "New Record!\n",
            "Epoch  26/30 Batch   60/78 - Loss:  0.880, Seconds: 2.50\n",
            "Average loss for this update: 0.926\n",
            "No Improvement.\n",
            "Epoch  27/30 Batch   20/78 - Loss:  1.028, Seconds: 2.69\n",
            "Average loss for this update: 1.028\n",
            "No Improvement.\n",
            "Epoch  27/30 Batch   40/78 - Loss:  0.942, Seconds: 2.59\n",
            "Average loss for this update: 0.869\n",
            "New Record!\n",
            "Epoch  27/30 Batch   60/78 - Loss:  0.829, Seconds: 2.78\n",
            "Average loss for this update: 0.873\n",
            "No Improvement.\n",
            "Epoch  28/30 Batch   20/78 - Loss:  0.985, Seconds: 2.63\n",
            "Average loss for this update: 0.982\n",
            "No Improvement.\n",
            "Epoch  28/30 Batch   40/78 - Loss:  0.892, Seconds: 2.48\n",
            "Average loss for this update: 0.82\n",
            "New Record!\n",
            "Epoch  28/30 Batch   60/78 - Loss:  0.789, Seconds: 2.47\n",
            "Average loss for this update: 0.824\n",
            "No Improvement.\n",
            "Epoch  29/30 Batch   20/78 - Loss:  0.954, Seconds: 2.54\n",
            "Average loss for this update: 0.949\n",
            "No Improvement.\n",
            "Epoch  29/30 Batch   40/78 - Loss:  0.863, Seconds: 2.44\n",
            "Average loss for this update: 0.801\n",
            "New Record!\n",
            "Epoch  29/30 Batch   60/78 - Loss:  0.767, Seconds: 2.49\n",
            "Average loss for this update: 0.796\n",
            "New Record!\n",
            "Epoch  30/30 Batch   20/78 - Loss:  0.923, Seconds: 2.91\n",
            "Average loss for this update: 0.92\n",
            "No Improvement.\n",
            "Epoch  30/30 Batch   40/78 - Loss:  0.828, Seconds: 2.44\n",
            "Average loss for this update: 0.765\n",
            "New Record!\n",
            "Epoch  30/30 Batch   60/78 - Loss:  0.729, Seconds: 2.57\n",
            "Average loss for this update: 0.754\n",
            "New Record!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ly9h8z40o8PC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l-1fQqaihBOu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clean_train_plot_top10 = clean_training_plot[0:10]\n",
        "clean_train_tag_top10 = clean_training_tag[0:10]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ja_SGEF7o9fp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_to_seq(text):\n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iFn-aXOphBLL",
        "colab_type": "code",
        "outputId": "90585191-1c8a-4a81-fa4f-4db864fe1ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "output_summaries = []\n",
        "checkpoint = \"./best_model.ckpt\"\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    \n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    \n",
        "    for i in range(0, len(clean_train_tag_top10)):\n",
        "        text = text_to_seq(clean_train_plot_top10[i])\n",
        "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(10,20)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "        temp = \" \".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
        "        output_summaries.append(temp)\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "reQ1WoTZpLQc",
        "colab_type": "code",
        "outputId": "a422ef42-17c3-48ae-acb5-eeb606de1053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.6/dist-packages (0.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zk08HFk3pXAJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "from rouge import Rouge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ka88F3x6hBHk",
        "colab_type": "code",
        "outputId": "7cc06ced-a6db-4f4e-9a2d-af256ea73d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(output_summaries, clean_train_tag_top10, avg = True)\n",
        "print(\"Training data scores:\",scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data scores: {'rouge-1': {'f': 0.187341266984638, 'p': 0.21761904761904766, 'r': 0.1692063492063492}, 'rouge-2': {'f': 0.10389610291044023, 'p': 0.12, 'r': 0.09166666666666666}, 'rouge-l': {'f': 0.1696042044174822, 'p': 0.20095238095238094, 'r': 0.1608730158730159}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JAJlpfbCr-EK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tagline Predictions**"
      ]
    },
    {
      "metadata": {
        "id": "f-mPd0tgtGsL",
        "colab_type": "code",
        "outputId": "68d4bffd-2f22-4c66-cc9a-fe8424a24129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Create your own review or use one from the dataset\n",
        "#input_sentence = \"Robbie, is set in 1998 and centres on a little girl, Gloria, who loves her nursemaid robot, Robbie. Her mother comes to believe that robots are unsafe, however, and Robbie is returned to the factory. Gloria is heartbroken. In an effort to show her that robots are machines, not people, her parents take her to see robots being assembled at a factory. One of the assembling robots is Robbie. Gloria endangers her life running to Robbie, and Robbie rescues Gloria, persuading Gloria's mother that robots can be trusted\"\n",
        "title = \"A Revenge Story\"\n",
        "input_sentence=\"When Bhallaladeva conspires against his brother to become the king of Mahishmati, he has him killed by Katappa and imprisons his wife. Years later, his brother's son returns to avenge his father's death.\"\n",
        "text = text_to_seq(input_sentence)\n",
        "#random = np.random.randint(0,len(clean_training_plot))\n",
        "#input_sentence = clean_training_plot[random]\n",
        "#text = text_to_seq(clean_training_plot[random])\n",
        "\n",
        "checkpoint = \"./best_model.ckpt\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    \n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    \n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Title:', title)\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
            "Original Title: A Revenge Story\n",
            "\n",
            "Text\n",
            "  Word Ids:    [33109, 17558, 1372, 140, 135, 33109, 1925, 33109, 12079, 1178, 411, 1165, 1372, 483, 1809, 1767, 482, 445]\n",
            "  Input Words: <UNK> conspires brother become king <UNK> killed <UNK> imprisons wife years later brother son returns avenge father death\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [87, 3869, 55, 3550, 1030]\n",
            "  Response Words: an bold tale unlike betrayal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8t7ziteZuenB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aWPzoPw0sPBH",
        "colab_type": "code",
        "outputId": "9f2927d5-0506-4ecc-dd59-518f418bd6b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "input_sentence = \"Hi This is Surekha from Hyderabad.I love irobot movie and the plot is as follows.A robot may not injure a human being or, through inaction, allow a human being to come to harm\"\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "checkpoint = \"./best_model.ckpt\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    \n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    \n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "#print('Original Text:', movie_training_data.movie_plot[random])\n",
        "#print('Original summary:', movie_training_data.tagLine[random])#clean_summaries[random]\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
            "\n",
            "Text\n",
            "  Word Ids:    [2841, 33109, 33109, 83, 33109, 6, 1782, 6343, 7046, 747, 33109, 65, 33109, 1908, 65, 25, 1665]\n",
            "  Input Words: hi <UNK> <UNK> love <UNK> movie plot follows robot may <UNK> human <UNK> allow human come harm\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [1283, 2810, 2811, 192, 20, 1002]\n",
            "  Response Words: super stuntman supermodel feel on inside\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CwIWAH2zsld0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Mistakes:1)Names are becoming unownk for local names and places\n",
        "                  2)Repetation of words \n",
        "                  3)Not exact prediction because restriction on Tagline length and Plot\n",
        "                 "
      ]
    },
    {
      "metadata": {
        "id": "kxpCbMJAnhA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Translation** **to** **Indian** **Languages**"
      ]
    },
    {
      "metadata": {
        "id": "4f7rLQeEwp6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output=format(\" \".join([int_to_vocab[i] for i in answer_logits]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QnGmvMC-nAEE",
        "colab_type": "code",
        "outputId": "6e1221b0-c387-4e20-a543-77e71d45e12f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "ipython = get_ipython()\n",
        "\n",
        "ipython.magic(\"sx pip install googletrans\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Requirement already satisfied: googletrans in /usr/local/lib/python3.6/dist-packages (2.4.0)',\n",
              " 'Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletrans) (2.21.0)',\n",
              " 'Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2.8)',\n",
              " 'Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (1.24.2)',\n",
              " 'Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2019.3.9)',\n",
              " 'Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (3.0.4)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "metadata": {
        "id": "EK8dFMlPnBAK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "def Translate_To_hindi(output):\n",
        "    '''\n",
        "    This function takes in the below parameters \n",
        "    summary = Movie summary/plot\n",
        "    To translate the summary to English language\n",
        "    '''\n",
        "    translator = Translator()\n",
        "    translation=translator.translate(output,dest='hindi') \n",
        "    return translation.text\n",
        "def Translate_To_telugu(output):\n",
        "    '''\n",
        "    This function takes in the below parameters \n",
        "    summary = Movie summary/plot\n",
        "    To translate the summary to English language\n",
        "    '''\n",
        "    translator = Translator()\n",
        "    translation=translator.translate(output,dest='telugu') \n",
        "    return translation.text\n",
        "def Translate_To_tamil(output):\n",
        "    '''\n",
        "    This function takes in the below parameters \n",
        "    summary = Movie summary/plot\n",
        "    To translate the summary to English language\n",
        "    '''\n",
        "    translator = Translator()\n",
        "    translation=translator.translate(output,dest='tamil') \n",
        "    return translation.text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ljL1D3cPnpZd",
        "colab_type": "code",
        "outputId": "fd33eccf-6a78-47cc-e4fd-e70a8a6583a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "summ=Translate_To_hindi(output)\n",
        "#summ=Translate_To_hindi(summ_in_english1)\n",
        "print(\"Hindi Translation:\",summ)\n",
        "summ=Translate_To_telugu(output)\n",
        "print(\"Telugu Translation:\",summ)\n",
        "summ=Translate_To_tamil(output)\n",
        "print(\"Tamil Translation:\",summ)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hindi Translation: सुपर स्टंटमैन सुपर मॉडल अंदर महसूस करता है\n",
            "Telugu Translation: లోపల సూపర్ స్టంట్మ్యాన్ సూపర్మోడల్ అనుభూతి\n",
            "Tamil Translation: சூப்பர் ஸ்டண்ட்மேன் சூப்பர்மாடல் உள்ளே உணர்கிறேன்\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EraOyhu-d3iK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zLXzz2-TGFtw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}