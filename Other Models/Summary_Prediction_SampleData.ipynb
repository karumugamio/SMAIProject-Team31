{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = '../input/glove6b50dtxt/glove.6B.50d.txt' \n",
    "# (glove data set from: https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('GloVe Loaded.')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "# Pre-trained GloVe embedding\n",
    "vocab,embd = loadGloVe(filename)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "word_vec_dim = len(embd[0]) # word_vec_dim = dimension of each word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with this code block 2\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk as nlp\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "summaries = []\n",
    "texts = []\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    print(text)\n",
    "    printable = set(string.printable)\n",
    "    print(printable)\n",
    "    return filter(lambda x: x in printable, text) #filter funny characters, if any. \n",
    "    \n",
    "\n",
    "with open('../input/foodreviews/Food-Reviews.csv') as csvfile:\n",
    "    \n",
    "    Reviews = csv.DictReader(csvfile)\n",
    "    for row in Reviews:\n",
    "        clean_text = row['Text']\n",
    "        clean_summary = row['Summary']\n",
    "        summaries.append(word_tokenize(clean_summary))\n",
    "        texts.append(word_tokenize(clean_text))\n",
    "print(\"Done with this code block 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE CLEANED & TOKENIZED TEXT:['Product', 'arrived', 'labeled', 'as', 'Jumbo', 'Salted', 'Peanuts', '...', 'the', 'peanuts', 'were', 'actually', 'small', 'sized', 'unsalted', '.', 'Not', 'sure', 'if', 'this', 'was', 'an', 'error', 'or', 'if', 'the', 'vendor', 'intended', 'to', 'represent', 'the', 'product', 'as', '``', 'Jumbo', \"''\", '.']\n",
      "\n",
      "SAMPLE CLEANED & TOKENIZED SUMMARY:['Not', 'as', 'Advertised']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = random.randint(0,len(texts)-1)\n",
    "\n",
    "print(\"SAMPLE CLEANED & TOKENIZED TEXT:\"+str(texts[index]))\n",
    "print(\"\\nSAMPLE CLEANED & TOKENIZED SUMMARY:\"+str(summaries[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_nearest_neighbour(x):\n",
    "    #returns array in embedding that's most similar (in terms of cosine similarity) to x\n",
    "        \n",
    "    xdoty = np.multiply(embedding,x)\n",
    "    xdoty = np.sum(xdoty,1)\n",
    "    xlen = np.square(x)\n",
    "    xlen = np.sum(xlen,0)\n",
    "    xlen = np.sqrt(xlen)\n",
    "    ylen = np.square(embedding)\n",
    "    ylen = np.sum(ylen,1)\n",
    "    ylen = np.sqrt(ylen)\n",
    "    xlenylen = np.multiply(xlen,ylen)\n",
    "    cosine_similarities = np.divide(xdoty,xlenylen)\n",
    "\n",
    "    return embedding[np.argmax(cosine_similarities)]\n",
    "    \n",
    "\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('unk')]\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    for x in range(0, len(embedding)):\n",
    "            if np.array_equal(embedding[x],np.asarray(vec)):\n",
    "                return vocab[x]\n",
    "    return vec2word(np_nearest_neighbour(np.asarray(vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'unk':\n",
      "\n",
      "[-7.9149e-01  8.6617e-01  1.1998e-01  9.2287e-04  2.7760e-01 -4.9185e-01\n",
      "  5.0195e-01  6.0792e-04 -2.5845e-01  1.7865e-01  2.5350e-01  7.6572e-01\n",
      "  5.0664e-01  4.0250e-01 -2.1388e-03 -2.8397e-01 -5.0324e-01  3.0449e-01\n",
      "  5.1779e-01  1.5090e-02 -3.5031e-01 -1.1278e+00  3.3253e-01 -3.5250e-01\n",
      "  4.1326e-02  1.0863e+00  3.3910e-02  3.3564e-01  4.9745e-01 -7.0131e-02\n",
      " -1.2192e+00 -4.8512e-01 -3.8512e-02 -1.3554e-01 -1.6380e-01  5.2321e-01\n",
      " -3.1318e-01 -1.6550e-01  1.1909e-01 -1.5115e-01 -1.5621e-01 -6.2655e-01\n",
      " -6.2336e-01 -4.2150e-01  4.1873e-01 -9.2472e-01  1.1049e+00 -2.9996e-01\n",
      " -6.3003e-03  3.9540e-01]\n"
     ]
    }
   ],
   "source": [
    "word = \"unk\"\n",
    "print(\"Vector representation of '\"+str(word)+\"':\\n\")\n",
    "print(word2vec(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REDUCE DATA (FOR SPEEDING UP THE NEXT STEPS)\n",
    "\n",
    "MAXIMUM_DATA_NUM = 50000\n",
    "\n",
    "texts = texts[0:MAXIMUM_DATA_NUM]\n",
    "summaries = summaries[0:MAXIMUM_DATA_NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_limit = []\n",
    "embd_limit = []\n",
    "\n",
    "i=0\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)\n",
    "                embd_limit.append(word2vec(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for summary in summaries:\n",
    "    for word in summary:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)\n",
    "                embd_limit.append(word2vec(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'eos' not in vocab_limit:\n",
    "    vocab_limit.append('eos')\n",
    "    embd_limit.append(word2vec('eos'))\n",
    "if 'unk' not in vocab_limit:\n",
    "    vocab_limit.append('unk')\n",
    "    embd_limit.append(word2vec('unk'))\n",
    "\n",
    "null_vector = np.zeros([word_vec_dim])\n",
    "\n",
    "vocab_limit.append('<PAD>')\n",
    "embd_limit.append(null_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_summaries = []\n",
    "\n",
    "for summary in summaries:\n",
    "    \n",
    "    vec_summary = []\n",
    "    \n",
    "    for word in summary:\n",
    "        vec_summary.append(word2vec(word))\n",
    "            \n",
    "    vec_summary.append(word2vec('eos'))\n",
    "    \n",
    "    vec_summary = np.asarray(vec_summary)\n",
    "    vec_summary = vec_summary.astype(np.float32)\n",
    "    \n",
    "    vec_summaries.append(vec_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_texts = []\n",
    "\n",
    "for text in texts:\n",
    "    \n",
    "    vec_text = []\n",
    "    \n",
    "    for word in text:\n",
    "        vec_text.append(word2vec(word))\n",
    "    \n",
    "    vec_text = np.asarray(vec_text)\n",
    "    vec_text = vec_text.astype(np.float32)\n",
    "    \n",
    "    vec_texts.append(vec_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_limit.append('<SOS>')\n",
    "embd_limit.append(np.zeros((word_vec_dim),dtype=np.float32))\n",
    "\n",
    "SOS = embd_limit[vocab_limit.index('<SOS>')]\n",
    "\n",
    "np_embd_limit = np.asarray(embd_limit,dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of dataset with summary length beyond 7: 10.0% \n",
      "Percentage of dataset with text length less that window size: 0.0% \n",
      "Percentage of dataset with text length more than 80: 20.0% \n"
     ]
    }
   ],
   "source": [
    "#DIAGNOSIS\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 7\n",
    "\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1>LEN:\n",
    "        count = count + 1\n",
    "print(\"Percentage of dataset with summary length beyond \"+str(LEN)+\": \"+str((count/len(vec_summaries))*100)+\"% \")\n",
    "\n",
    "count = 0\n",
    "\n",
    "D = 10 \n",
    "\n",
    "window_size = 2*D+1\n",
    "\n",
    "for text in vec_texts:\n",
    "    if len(text)<window_size+1:\n",
    "        count = count + 1\n",
    "print(\"Percentage of dataset with text length less that window size: \"+str((count/len(vec_texts))*100)+\"% \")\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 80\n",
    "\n",
    "for text in vec_texts:\n",
    "    if len(text)>LEN:\n",
    "        count = count + 1\n",
    "print(\"Percentage of dataset with text length more than \"+str(LEN)+\": \"+str((count/len(vec_texts))*100)+\"% \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SUMMARY_LEN = 7\n",
    "MAX_TEXT_LEN = 80\n",
    "\n",
    "#D is a major hyperparameters. Windows size for local attention will be 2*D+1\n",
    "D = 10\n",
    "\n",
    "window_size = 2*D+1\n",
    "\n",
    "#REMOVE DATA WHOSE SUMMARIES ARE TOO BIG\n",
    "#OR WHOSE TEXT LENGTH IS TOO BIG\n",
    "#OR WHOSE TEXT LENGTH IS SMALLED THAN WINDOW SIZE\n",
    "\n",
    "vec_summaries_reduced = []\n",
    "vec_texts_reduced = []\n",
    "\n",
    "i = 0\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1<=MAX_SUMMARY_LEN and len(vec_texts[i])>=window_size and len(vec_texts[i])<=MAX_TEXT_LEN:\n",
    "        vec_summaries_reduced.append(summary)\n",
    "        vec_texts_reduced.append(vec_texts[i])\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "train_len = int((.7)*len(vec_summaries_reduced))\n",
    "\n",
    "train_texts = vec_texts_reduced[0:train_len]\n",
    "train_summaries = vec_summaries_reduced[0:train_len]\n",
    "\n",
    "val_len = int((.15)*len(vec_summaries_reduced))\n",
    "\n",
    "val_texts = vec_texts_reduced[train_len:train_len+val_len]\n",
    "val_summaries = vec_summaries_reduced[train_len:train_len+val_len]\n",
    "\n",
    "test_texts = vec_texts_reduced[train_len+val_len:len(vec_summaries_reduced)]\n",
    "test_summaries = vec_summaries_reduced[train_len+val_len:len(vec_summaries_reduced)]\n",
    "print(train_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_out(output_text):\n",
    "    output_len = len(output_text)\n",
    "    transformed_output = np.zeros([output_len],dtype=np.int32)\n",
    "    for i in range(0,output_len):\n",
    "        transformed_output[i] = vocab_limit.index(vec2word(output_text[i]))\n",
    "    return transformed_output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "learning_rate = 0.003\n",
    "K = 5\n",
    "vocab_len = len(vocab_limit)\n",
    "training_iters = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "#placeholders\n",
    "tf_text = tf.placeholder(tf.float32, [None,word_vec_dim])\n",
    "tf_seq_len = tf.placeholder(tf.int32)\n",
    "tf_summary = tf.placeholder(tf.int32,[None])\n",
    "tf_output_len = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_encoder(inp,hidden,cell,\n",
    "                    wf,uf,bf,\n",
    "                    wi,ui,bi,\n",
    "                    wo,uo,bo,\n",
    "                    wc,uc,bc,\n",
    "                    Wattention,seq_len,inp_dim):\n",
    "\n",
    "    Wattention = tf.nn.softmax(Wattention,0)\n",
    "    hidden_forward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    i=0\n",
    "    j=K\n",
    "    \n",
    "    def cond(i,j,hidden,cell,hidden_forward,hidden_residuals):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,j,hidden,cell,hidden_forward,hidden_residuals):\n",
    "        \n",
    "        x = tf.reshape(inp[i],[1,inp_dim])\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n",
    "        RRA = tf.reshape(RRA,[1,hidden_size])\n",
    "        \n",
    "        # LSTM with RRA\n",
    "        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "        \n",
    "        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n",
    "                                   lambda: hidden_residuals,\n",
    "                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n",
    "\n",
    "        hidden_forward = hidden_forward.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i+1,j+1,hidden,cell,hidden_forward,hidden_residuals\n",
    "    \n",
    "    _,_,_,_,hidden_forward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_forward,hidden_residuals])\n",
    "    \n",
    "    hidden_residuals.close().mark_used()\n",
    "    \n",
    "    return hidden_forward.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_encoder(inp,hidden,cell,\n",
    "                     wf,uf,bf,\n",
    "                     wi,ui,bi,\n",
    "                     wo,uo,bo,\n",
    "                     wc,uc,bc,\n",
    "                     Wattention,seq_len,inp_dim):\n",
    "    \n",
    "    Wattention = tf.nn.softmax(Wattention,0)\n",
    "    hidden_backward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    i=seq_len-1\n",
    "    j=K\n",
    "    \n",
    "    def cond(i,j,hidden,cell,hidden_backward,hidden_residuals):\n",
    "        return i > -1\n",
    "    \n",
    "    def body(i,j,hidden,cell,hidden_backward,hidden_residuals):\n",
    "        \n",
    "        x = tf.reshape(inp[i],[1,inp_dim])\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n",
    "        RRA = tf.reshape(RRA,[1,hidden_size])\n",
    "        \n",
    "        # LSTM with RRA\n",
    "        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "\n",
    "        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n",
    "                                   lambda: hidden_residuals,\n",
    "                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n",
    "        \n",
    "        hidden_backward = hidden_backward.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i-1,j+1,hidden,cell,hidden_backward,hidden_residuals\n",
    "    \n",
    "    _,_,_,_,hidden_backward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_backward,hidden_residuals])\n",
    "\n",
    "    hidden_residuals.close().mark_used()\n",
    "    \n",
    "    return hidden_backward.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x,hidden,cell,\n",
    "            wf,uf,bf,\n",
    "            wi,ui,bi,\n",
    "            wo,uo,bo,\n",
    "            wc,uc,bc,RRA):\n",
    "    \n",
    "    # LSTM with RRA\n",
    "    fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "    ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "    og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "    cell_next = tf.multiply(fg,cell) + tf.multiply(ig,tf.tanh( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "    hidden_next = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "    \n",
    "    return hidden_next,cell_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(hs,ht,Wa,seq_len):\n",
    "    return tf.reshape(tf.matmul(tf.matmul(hs,Wa),tf.transpose(ht)),[seq_len])\n",
    "\n",
    "def align(hs,ht,Wp,Vp,Wa,tf_seq_len):\n",
    "   \n",
    "    pd = tf.TensorArray(size=(2*D+1),dtype=tf.float32)\n",
    "    \n",
    "    positions = tf.cast(tf_seq_len-1-2*D,dtype=tf.float32)\n",
    "    \n",
    "    sigmoid_multiplier = tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(ht,Wp)),Vp))\n",
    "    sigmoid_multiplier = tf.reshape(sigmoid_multiplier,[])\n",
    "    \n",
    "    pt_float = positions*sigmoid_multiplier\n",
    "    \n",
    "    pt = tf.cast(pt_float,tf.int32)\n",
    "    pt = pt+D #center to window\n",
    "    \n",
    "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
    "    \n",
    "    i = 0\n",
    "    pos = pt - D\n",
    "    \n",
    "    def cond(i,pos,pd):\n",
    "        \n",
    "        return i < (2*D+1)\n",
    "                      \n",
    "    def body(i,pos,pd):\n",
    "        \n",
    "        comp_1 = tf.cast(tf.square(pos-pt),tf.float32)\n",
    "        comp_2 = tf.cast(2*tf.square(sigma),tf.float32)\n",
    "            \n",
    "        pd = pd.write(i,tf.exp(-(comp_1/comp_2)))\n",
    "            \n",
    "        return i+1,pos+1,pd\n",
    "                      \n",
    "    i,pos,pd = tf.while_loop(cond,body,[i,pos,pd])\n",
    "    \n",
    "    local_hs = hs[(pt-D):(pt+D+1)]\n",
    "    \n",
    "    normalized_scores = tf.nn.softmax(score(local_hs,ht,Wa,2*D+1))\n",
    "    \n",
    "    pd=pd.stack()\n",
    "    \n",
    "    G = tf.multiply(normalized_scores,pd)\n",
    "    G = tf.reshape(G,[2*D+1,1])\n",
    "    \n",
    "    return G,pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(tf_text,tf_seq_len,tf_output_len):\n",
    "    \n",
    "    #PARAMETERS\n",
    "    \n",
    "    #1.1 FORWARD ENCODER PARAMETERS\n",
    "    \n",
    "    initial_hidden_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    cell_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    wf_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uf_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bf_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wi_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    ui_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bi_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wo_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uo_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bo_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wc_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uc_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bc_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    Wattention_f = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "                               \n",
    "    #1.2 BACKWARD ENCODER PARAMETERS\n",
    "    \n",
    "    initial_hidden_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    cell_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    wf_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uf_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bf_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wi_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    ui_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bi_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wo_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uo_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bo_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wc_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uc_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bc_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    Wattention_b = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "    \n",
    "    #2 ATTENTION PARAMETERS\n",
    "    \n",
    "    Wp = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,50],stddev=0.01))\n",
    "    Vp = tf.Variable(tf.truncated_normal(shape=[50,1],stddev=0.01))\n",
    "    Wa = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,2*hidden_size],stddev=0.01))\n",
    "    Wc = tf.Variable(tf.truncated_normal(shape=[4*hidden_size,2*hidden_size],stddev=0.01))\n",
    "    \n",
    "    #3 DECODER PARAMETERS\n",
    "    \n",
    "    Ws = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,vocab_len],stddev=0.01))\n",
    "    \n",
    "    cell_d = tf.zeros([1,2*hidden_size],dtype=tf.float32)\n",
    "    wf_d = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,2*hidden_size],stddev=0.01))\n",
    "    uf_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bf_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wi_d = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,2*hidden_size],stddev=0.01))\n",
    "    ui_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bi_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wo_d = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,2*hidden_size],stddev=0.01))\n",
    "    uo_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bo_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wc_d = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,2*hidden_size],stddev=0.01))\n",
    "    uc_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bc_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals_d = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals_d = hidden_residuals_d.unstack(tf.zeros([K,2*hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    Wattention_d = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "    \n",
    "    output = tf.TensorArray(size=tf_output_len,dtype=tf.float32)\n",
    "                               \n",
    "    #BI-DIRECTIONAL LSTM\n",
    "                               \n",
    "    hidden_forward = forward_encoder(tf_text,\n",
    "                                     initial_hidden_f,cell_f,\n",
    "                                     wf_f,uf_f,bf_f,\n",
    "                                     wi_f,ui_f,bi_f,\n",
    "                                     wo_f,uo_f,bo_f,\n",
    "                                     wc_f,uc_f,bc_f,\n",
    "                                     Wattention_f,\n",
    "                                     tf_seq_len,\n",
    "                                     word_vec_dim)\n",
    "    \n",
    "    hidden_backward = backward_encoder(tf_text,\n",
    "                                     initial_hidden_b,cell_b,\n",
    "                                     wf_b,uf_b,bf_b,\n",
    "                                     wi_b,ui_b,bi_b,\n",
    "                                     wo_b,uo_b,bo_b,\n",
    "                                     wc_b,uc_b,bc_b,\n",
    "                                     Wattention_b,\n",
    "                                     tf_seq_len,\n",
    "                                     word_vec_dim)\n",
    "    \n",
    "    encoded_hidden = tf.concat([hidden_forward,hidden_backward],1)\n",
    "    \n",
    "    #ATTENTION MECHANISM AND DECODER\n",
    "    \n",
    "    decoded_hidden = encoded_hidden[0]\n",
    "    decoded_hidden = tf.reshape(decoded_hidden,[1,2*hidden_size])\n",
    "    Wattention_d_normalized = tf.nn.softmax(Wattention_d)\n",
    "    tf_embd_limit = tf.convert_to_tensor(np_embd_limit)\n",
    "    \n",
    "    y = tf.convert_to_tensor(SOS) #inital decoder token <SOS> vector\n",
    "    y = tf.reshape(y,[1,word_vec_dim])\n",
    "    \n",
    "    j=K\n",
    "    \n",
    "    hidden_residuals_stack = hidden_residuals_d.stack()\n",
    "    \n",
    "    RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n",
    "    RRA = tf.reshape(RRA,[1,2*hidden_size])\n",
    "    \n",
    "    decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n",
    "                                  wf_d,uf_d,bf_d,\n",
    "                                  wi_d,ui_d,bf_d,\n",
    "                                  wo_d,uo_d,bf_d,\n",
    "                                  wc_d,uc_d,bc_d,\n",
    "                                  RRA)\n",
    "    decoded_hidden = decoded_hidden_next\n",
    "    \n",
    "    hidden_residuals_d = hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size]))\n",
    "    \n",
    "    j=j+1\n",
    "                           \n",
    "    i=0\n",
    "    \n",
    "    def attention_decoder_cond(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):\n",
    "        return i < tf_output_len\n",
    "    \n",
    "    def attention_decoder_body(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):\n",
    "        \n",
    "        #LOCAL ATTENTION\n",
    "        \n",
    "        G,pt = align(encoded_hidden,decoded_hidden,Wp,Vp,Wa,tf_seq_len)\n",
    "        local_encoded_hidden = encoded_hidden[pt-D:pt+D+1]\n",
    "        weighted_encoded_hidden = tf.multiply(local_encoded_hidden,G)\n",
    "        context_vector = tf.reduce_sum(weighted_encoded_hidden,0)\n",
    "        context_vector = tf.reshape(context_vector,[1,2*hidden_size])\n",
    "        \n",
    "        attended_hidden = tf.tanh(tf.matmul(tf.concat([context_vector,decoded_hidden],1),Wc))\n",
    "        \n",
    "        #DECODER\n",
    "        \n",
    "        y = tf.matmul(attended_hidden,Ws)\n",
    "        \n",
    "        output = output.write(i,tf.reshape(y,[vocab_len]))\n",
    "        #Save probability distribution as output\n",
    "        \n",
    "        y = tf.nn.softmax(y)\n",
    "        \n",
    "        y_index = tf.cast(tf.argmax(tf.reshape(y,[vocab_len])),tf.int32)\n",
    "        y = tf_embd_limit[y_index]\n",
    "        y = tf.reshape(y,[1,word_vec_dim])\n",
    "        \n",
    "        #setting next decoder input token as the word_vector of maximum probability \n",
    "        #as found from previous attention-decoder output.\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals_d.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n",
    "        RRA = tf.reshape(RRA,[1,2*hidden_size])\n",
    "        \n",
    "        decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n",
    "                                  wf_d,uf_d,bf_d,\n",
    "                                  wi_d,ui_d,bf_d,\n",
    "                                  wo_d,uo_d,bf_d,\n",
    "                                  wc_d,uc_d,bc_d,\n",
    "                                  RRA)\n",
    "        \n",
    "        decoded_hidden = decoded_hidden_next\n",
    "        \n",
    "        hidden_residuals_d = tf.cond(tf.equal(j,tf_output_len-1+K+1), #(+1 for <SOS>)\n",
    "                                   lambda: hidden_residuals_d,\n",
    "                                   lambda: hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size])))\n",
    "        \n",
    "        return i+1,j+1,decoded_hidden,cell_d,hidden_residuals_d,output\n",
    "    \n",
    "    i,j,decoded_hidden,cell_d,hidden_residuals_d,output = tf.while_loop(attention_decoder_cond,\n",
    "                                            attention_decoder_body,\n",
    "                                            [i,j,decoded_hidden,cell_d,hidden_residuals_d,output])\n",
    "    hidden_residuals_d.close().mark_used()\n",
    "    \n",
    "    output = output.stack()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "output = model(tf_text,tf_seq_len,tf_output_len)\n",
    "\n",
    "#OPTIMIZER\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=tf_summary))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#PREDICTION\n",
    "\n",
    "pred = tf.TensorArray(size=tf_output_len,dtype=tf.int32)\n",
    "\n",
    "i=0\n",
    "\n",
    "def cond_pred(i,pred):\n",
    "    return i<tf_output_len\n",
    "def body_pred(i,pred):\n",
    "    pred = pred.write(i,tf.cast(tf.argmax(output[i]),tf.int32))\n",
    "    return i+1,pred\n",
    "\n",
    "i,pred = tf.while_loop(cond_pred,body_pred,[i,pred]) \n",
    "\n",
    "prediction = pred.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 0\n",
      "Training input sequence length: 51\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "unk have bought several of the unk canned dog food products and have found them all to be of good quality. unk product looks more like a stew than a processed meat and it smells better. unk unk is finicky and she appreciates this product better than most.\n",
      "\n"
     ]
    },
    {
     "ename": "AlreadyExistsError",
     "evalue": "Resource __per_step_2/while/ArithmeticOptimizer/AddOpsRewrite_add_2/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node while/ArithmeticOptimizer/AddOpsRewrite_add_2/tmp_var}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: Resource __per_step_2/while/ArithmeticOptimizer/AddOpsRewrite_add_2/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node while/ArithmeticOptimizer/AddOpsRewrite_add_2/tmp_var}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-67e74fe2a57f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m                                                     \u001b[0mtf_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                                                     \u001b[0mtf_summary\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                                                     tf_output_len: len(train_out)})\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: Resource __per_step_2/while/ArithmeticOptimizer/AddOpsRewrite_add_2/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node while/ArithmeticOptimizer/AddOpsRewrite_add_2/tmp_var}}]]"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from __future__ import print_function\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 0   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    display_step = 1\n",
    "    \n",
    "    while step < training_iters:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "           \n",
    "        for i in range(0,train_len):\n",
    "            \n",
    "            train_out = transform_out(train_summaries[i][0:len(train_summaries[i])-1])\n",
    "            \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nIteration: \"+str(i))\n",
    "                print(\"Training input sequence length: \"+str(len(train_texts[i])))\n",
    "                print(\"Training target outputs sequence length: \"+str(len(train_out)))\n",
    "            \n",
    "                print(\"\\nTEXT:\")\n",
    "                flag = 0\n",
    "                for vec in train_texts[i]:\n",
    "                    if vec2word(vec) in string.punctuation or flag==0:\n",
    "                        print(str(vec2word(vec)),end='')\n",
    "                    else:\n",
    "                        print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "\n",
    "\n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,pred = sess.run([optimizer,cost,prediction],feed_dict={tf_text: train_texts[i], \n",
    "                                                    tf_seq_len: len(train_texts[i]), \n",
    "                                                    tf_summary: train_out,\n",
    "                                                    tf_output_len: len(train_out)})\n",
    "            \n",
    "         \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nPREDICTED SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for index in pred:\n",
    "                    #if int(index)!=vocab_limit.index('eos'):\n",
    "                    if vocab_limit[int(index)] in string.punctuation or flag==0:\n",
    "                        print(str(vocab_limit[int(index)]),end='')\n",
    "                    else:\n",
    "                        print(\" \"+str(vocab_limit[int(index)]),end='')\n",
    "                    flag=1\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"ACTUAL SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for vec in train_summaries[i]:\n",
    "                    if vec2word(vec)!='eos':\n",
    "                        if vec2word(vec) in string.punctuation or flag==0:\n",
    "                            print(str(vec2word(vec)),end='')\n",
    "                        else:\n",
    "                            print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "                print(\"loss=\"+str(loss))\n",
    "            \n",
    "        step=step+1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
