{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictingMovie_tagline_BiRNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "KQgi5rlwLQoc",
        "colab_type": "code",
        "outputId": "f86b0375-df09-4afa-fcda-230b1cba8936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FDWr5w9EX-x9",
        "colab_type": "code",
        "outputId": "6d3bf16e-1b6c-4bc5-e660-55aec12bb369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2wXkBJwsQUiF",
        "colab_type": "code",
        "outputId": "b67cf7d0-2b3b-48b2-a347-a87dac4730cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o15gRUiULQow",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Insepcting the Data"
      ]
    },
    {
      "metadata": {
        "id": "8ZSnw1r-LQoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "movie_data = pd.read_csv(\"gdrive/My Drive/movies_text.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHAvy3bWLQpe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "movie_data.shape\n",
        "movie_data = movie_data.dropna()\n",
        "movie_data = movie_data.reset_index(drop=True)\n",
        "#movie_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2vtyxL_RgPaR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "movie_data.shape\n",
        "movie_data=movie_data.rename(columns = {\"Summary\": \"tagLine\",\"Text\":\"movie_plot\"})\n",
        "movie_data = pd.DataFrame(movie_data, columns=[\"movie_plot\",\"tagLine\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zkHn1e_BRfVj",
        "colab_type": "code",
        "outputId": "a615898b-41c4-4e74-f07a-937ce4b1922c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Actual Data :\",movie_data.shape)\n",
        "print(\"Train data:\",movie_data.iloc[:int(movie_data.shape[0]*0.6)].shape)\n",
        "print(\"Validation Data:\",movie_data.iloc[int(movie_data.shape[0]*0.6):int(movie_data.shape[0]*0.8)].shape)\n",
        "print(\"Test Data:\",movie_data.iloc[int(movie_data.shape[0]*0.80):].shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Data : (20404, 2)\n",
            "Train data: (12242, 2)\n",
            "Validation Data: (4081, 2)\n",
            "Test Data: (4081, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z6PtsTjChq5u",
        "colab_type": "code",
        "outputId": "0517fb81-0751-4a94-8d26-85f3df777ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "movie_training_data=movie_data.iloc[:int(movie_data.shape[0]*0.6)].reset_index(drop=True)\n",
        "movie_val_data=movie_data.iloc[int(movie_data.shape[0]*0.6):int(movie_data.shape[0]*0.8)].reset_index(drop=True)\n",
        "movie_test_data=movie_data.iloc[int(movie_data.shape[0]*0.85):].reset_index(drop=True)\n",
        "   \n",
        "\n",
        "movie_training_data = movie_training_data.sample(frac=1).reset_index(drop=True)\n",
        "movie_val_data = movie_val_data.sample(frac=1).reset_index(drop=True)\n",
        "movie_test_data = movie_test_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "#print(movie_training_data.shape)\n",
        "#print(movie_val_data.shape)\n",
        "#print(movie_test_data.shape)\n",
        "\n",
        "movie_data.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_plot</th>\n",
              "      <th>tagLine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When siblings Judy and Peter discover an encha...</td>\n",
              "      <td>Roll the dice and unleash the excitement!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A family wedding reignites the ancient feud be...</td>\n",
              "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
              "      <td>Friends are the people who let you be yourself...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Just when George Banks has recovered from his ...</td>\n",
              "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n",
              "      <td>A Los Angeles Crime Saga</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          movie_plot  \\\n",
              "0  When siblings Judy and Peter discover an encha...   \n",
              "1  A family wedding reignites the ancient feud be...   \n",
              "2  Cheated on, mistreated and stepped on, the wom...   \n",
              "3  Just when George Banks has recovered from his ...   \n",
              "4  Obsessive master thief, Neil McCauley leads a ...   \n",
              "\n",
              "                                             tagLine  \n",
              "0          Roll the dice and unleash the excitement!  \n",
              "1  Still Yelling. Still Fighting. Still Ready for...  \n",
              "2  Friends are the people who let you be yourself...  \n",
              "3  Just When His World Is Back To Normal... He's ...  \n",
              "4                           A Los Angeles Crime Saga  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "5c81RLLPLQpy",
        "colab_type": "code",
        "outputId": "5e41e1f6-983b-410c-d692-781e55c871b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "cell_type": "code",
      "source": [
        "# Inspecting Data\n",
        "for i in range(10):\n",
        "    print(\"Before Data Cleaning movie_Plot #\",i+1)\n",
        "    print(movie_training_data.movie_plot[i])\n",
        "    print(\"Tagline\")\n",
        "    print(movie_training_data.tagLine[i])\n",
        "    \n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before Data Cleaning movie_Plot # 1\n",
            "A battle ensues among two government spy teams in an underground facility after their boss is assassinated.\n",
            "Tagline\n",
            "Two rival teams of assassins. One killer day at the office.\n",
            "\n",
            "Before Data Cleaning movie_Plot # 2\n",
            "Bettie Page grew up in a conservative religious family in Tennessee and became a photo model sensation in 1950s New York. Bettie's legendary pin-up photos made her the target of a Senate investigation into pornography, and transformed her into an erotic icon who continues to enthrall fans to this day.\n",
            "Tagline\n",
            "The Pin-Up Sensation That Shocked The Nation.\n",
            "\n",
            "Before Data Cleaning movie_Plot # 3\n",
            "Two crew members wake up on an abandoned spacecraft with no idea who they are, how long they've been asleep, or what their mission is. The two soon discover they're actually not alone – and the reality of their situation is more horrifying than they could have imagined.\n",
            "Tagline\n",
            "Don't fear the end of the world. Fear what happens next.\n",
            "\n",
            "Before Data Cleaning movie_Plot # 4\n",
            "Charlie is an immigrant who endures a challenging voyage and gets into trouble as soon as he arrives in America.\n",
            "Tagline\n",
            "\"The Tramp\" arrives in New York\n",
            "\n",
            "Before Data Cleaning movie_Plot # 5\n",
            "Here is the story of a beautiful, proud and tough loner, a sailor named Querelle, whose commanding officer Seblon worships and desires him from afar. Querelle turns on his drugs-smuggling partner and murders him. He then goes to a notorious brothel run by the rapacious Lysiane, who leads Querelle into his first homosexual encounter. Then, Querelle has become vulnerable and soft, and soon the once powerful object of passion comes to belong to Seblon.\n",
            "Tagline\n",
            "It will take you into a surreal world of passion and sexuality, further than most would dare to go.\n",
            "\n",
            "Before Data Cleaning movie_Plot # 6\n",
            "Bessie and Winston \"Slug\" Winters are married coaches whose mission is to whip their college football team into shape. Just in time, they discover a hillbilly farmhand and his sister. But the hillbilly farmhand's ability to throw melons enables him to become their star passing ace.\n",
            "Tagline\n",
            "A BAREFOOT HILLBILLY FROM TEXAS DOES HIS STUFF IN THE YALE BOWL!\n",
            "\n",
            "Before Data Cleaning movie_Plot # 7\n",
            "Five friends break into a closed corn maze in the middle of the night and decide to play a harmless game of tag. Little do they know that a psychopathic killer has decided to play along. As they wander aimlessly through the maze the murderer follows closely behind, taunting them and watching their every move. The game turns deadly when the kids decide to separate and weaken their chances of survival. When the mutilated body of the maze owner is found they realize that something is terribly wrong. As they race to find the entrance of the maze, the murderer cleverly forces them to follow the path that he wants. Manipulating everyone to his vicious will, the killer taunts his victims and leads them further into the depths of the maze. After succumbing to hours of torture will anyone make it out of the maze...alive?\n",
            "Tagline\n",
            "This is no field of dreams!\n",
            "\n",
            "Before Data Cleaning movie_Plot # 8\n",
            "Apartment building superintendent Cleveland Heep rescues what he thinks is a young woman from the pool he maintains. When he discovers that she is actually a character from a bedtime story who is trying to make the journey back to her home, he works with his tenants to protect his new friend from the creatures that are determined to keep her in our world.\n",
            "Tagline\n",
            "Time is running out for a happy ending.\n",
            "\n",
            "Before Data Cleaning movie_Plot # 9\n",
            "B.G. Bruno, a rich bachelor, the head of a successful greeting-card company in Scotland, is essentially a kind man but respectable to the point of stodginess and extreme stuffiness. An American troupe visiting Edinburgh wants to produce a musical in town but has trouble getting backers. Bruno meets several of the leading ladies of the show; through a misunderstanding he doesn't correct they think that he's a newspaper reporter. He falls in love with one of the women, who reciprocates; he grows more lively and friendly, to the surprise of his employees. After a series of mishaps and comic incidents comes a happy ending: a successful show and true love.\n",
            "Tagline\n",
            "Love...Fun...Youth...Set to Music!\n",
            "\n",
            "Before Data Cleaning movie_Plot # 10\n",
            "In LA's Fairfax district, where ethnic groups abound, four households celebrate Thanksgiving amidst family tensions. In the Nguyen family, the children's acculturation and immigrant parents' fears collide. In the Avila family, Isabel's son has invited her estranged husband to their family dinner. Audrey and Ron Williams want to keep their own family's ruptures secret from Ron's visiting mother. In the Seelig household, Herb and Ruth are unwilling to discuss openly their grown daughter's living with her lover, Carla. Around each table, things come to a head. A gun, an affair, a boyfriend, and a pregnancy precipitate crises forcing each family to find its center.\n",
            "Tagline\n",
            "Thanksgiving. A celebration of food, tradition and relative insanity.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IKiV-eDeLQp9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Wrangling**"
      ]
    },
    {
      "metadata": {
        "id": "x46-twCOLQqA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reference for  list of contractions in English took from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g1teGf2TLQqJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Cleaning the data\n",
        "def clean_text(text, remove_stopwords = True):\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # using contractions to replace short with their longer forms \n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    \n",
        "    # Extraction of unimportant characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    \n",
        "    #removing stop words(optional if not nltk library)\n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pP8Jq6T7S8CS",
        "colab_type": "code",
        "outputId": "b8a8174b-6179-450f-e852-4a2b1d67f3cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#nltk library for removing Stop words\n",
        "import nltk\n",
        "#nltk.download()\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "DqWB7N4cLQqV",
        "colab_type": "code",
        "outputId": "9ddc2adc-4dfb-4070-bd6e-51874a62e6f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# Clean the Training Taglines and Movie Plots \n",
        "clean_training_plot = []\n",
        "for movieplot_train in movie_training_data.movie_plot:\n",
        "    clean_training_plot.append(clean_text(movieplot_train))\n",
        "print(\"Training movie plots are complete.\")\n",
        "\n",
        "\n",
        "clean_training_tag = []\n",
        "for tagline_train in movie_training_data.tagLine:\n",
        "    clean_training_tag.append(clean_text(tagline_train, remove_stopwords=False))\n",
        "print(\"Training Taglines are complete.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Clean the Validation Taglines and Movie Plots \n",
        "clean_val_plot = []\n",
        "for movieplot_val in movie_val_data.movie_plot:\n",
        "    clean_val_plot.append(clean_text(movieplot_val))\n",
        "print(\"Validation movie plots are complete.\")\n",
        "\n",
        "clean_val_tag = []\n",
        "for tagline_val in movie_val_data.tagLine:\n",
        "    clean_val_tag.append(clean_text(tagline_val, remove_stopwords=False))\n",
        "print(\"Validation Taglines are complete.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Clean the Testing Taglines and Movie Plots \n",
        "clean_test_plot = []\n",
        "for movieplot_test in movie_test_data.movie_plot:\n",
        "    clean_test_plot.append(clean_text(movieplot_test))\n",
        "print(\"Testing movie plots are complete.\")\n",
        "\n",
        "\n",
        "\n",
        "clean_test_tag = []\n",
        "for tagline_test in movie_test_data.tagLine:\n",
        "    clean_test_tag.append(clean_text(tagline_test, remove_stopwords=False))\n",
        "print(\"Testing Taglines are complete.\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training movie plots are complete.\n",
            "Training Taglines are complete.\n",
            "Validation movie plots are complete.\n",
            "Validation Taglines are complete.\n",
            "Testing movie plots are complete.\n",
            "Testing Taglines are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EghwEfRKLQqg",
        "colab_type": "code",
        "outputId": "2634b14f-91c9-45bb-802d-59279ef30f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "cell_type": "code",
      "source": [
        "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
        "for i in range(10):\n",
        "    print(\"After Data Cleaning Movie_Plot #\",i+1)\n",
        "    print(clean_training_plot[i])\n",
        "    print(\"Tagline\")\n",
        "    print(clean_training_tag[i])\n",
        "    \n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After Data Cleaning Movie_Plot # 1\n",
            "battle ensues among two government spy teams underground facility boss assassinated\n",
            "Tagline\n",
            "two rival teams of assassins  one killer day at the office \n",
            "\n",
            "After Data Cleaning Movie_Plot # 2\n",
            "bettie page grew conservative religious family tennessee became photo model sensation 1950s new york bettie legendary pin photos made target senate investigation pornography transformed erotic icon continues enthrall fans day\n",
            "Tagline\n",
            "the pin up sensation that shocked the nation \n",
            "\n",
            "After Data Cleaning Movie_Plot # 3\n",
            "two crew members wake abandoned spacecraft idea long asleep mission two soon discover actually alone – reality situation horrifying could imagined\n",
            "Tagline\n",
            "do not fear the end of the world  fear what happens next \n",
            "\n",
            "After Data Cleaning Movie_Plot # 4\n",
            "charlie immigrant endures challenging voyage gets trouble soon arrives america\n",
            "Tagline\n",
            " the tramp  arrives in new york\n",
            "\n",
            "After Data Cleaning Movie_Plot # 5\n",
            "story beautiful proud tough loner sailor named querelle whose commanding officer seblon worships desires afar querelle turns drugs smuggling partner murders goes notorious brothel run rapacious lysiane leads querelle first homosexual encounter querelle become vulnerable soft soon powerful object passion comes belong seblon\n",
            "Tagline\n",
            "it will take you into a surreal world of passion and sexuality  further than most would dare to go \n",
            "\n",
            "After Data Cleaning Movie_Plot # 6\n",
            "bessie winston slug winters married coaches whose mission whip college football team shape time discover hillbilly farmhand sister hillbilly farmhand ability throw melons enables become star passing ace\n",
            "Tagline\n",
            "a barefoot hillbilly from texas does his stuff in the yale bowl \n",
            "\n",
            "After Data Cleaning Movie_Plot # 7\n",
            "five friends break closed corn maze middle night decide play harmless game tag little know psychopathic killer decided play along wander aimlessly maze murderer follows closely behind taunting watching every move game turns deadly kids decide separate weaken chances survival mutilated body maze owner found realize something terribly wrong race find entrance maze murderer cleverly forces follow path wants manipulating everyone vicious killer taunts victims leads depths maze succumbing hours torture anyone make maze alive\n",
            "Tagline\n",
            "this is no field of dreams \n",
            "\n",
            "After Data Cleaning Movie_Plot # 8\n",
            "apartment building superintendent cleveland heep rescues thinks young woman pool maintains discovers actually character bedtime story trying make journey back home works tenants protect new friend creatures determined keep world\n",
            "Tagline\n",
            "time is running out for a happy ending \n",
            "\n",
            "After Data Cleaning Movie_Plot # 9\n",
            "b g bruno rich bachelor head successful greeting card company scotland essentially kind man respectable point stodginess extreme stuffiness american troupe visiting edinburgh wants produce musical town trouble getting backers bruno meets several leading ladies show misunderstanding correct think newspaper reporter falls love one women reciprocates grows lively friendly surprise employees series mishaps comic incidents comes happy ending successful show true love\n",
            "Tagline\n",
            "love   fun   youth   set to music \n",
            "\n",
            "After Data Cleaning Movie_Plot # 10\n",
            "la fairfax district ethnic groups abound four households celebrate thanksgiving amidst family tensions nguyen family children acculturation immigrant parents fears collide avila family isabel son invited estranged husband family dinner audrey ron williams want keep family ruptures secret ron visiting mother seelig household herb ruth unwilling discuss openly grown daughter living lover carla around table things come head gun affair boyfriend pregnancy precipitate crises forcing family find center\n",
            "Tagline\n",
            "thanksgiving  a celebration of food  tradition and relative insanity \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X8rS8qoCLQqv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#count of words\n",
        "def words_counter(count_d, text):\n",
        "    \n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_d:\n",
        "                count_d[word] = 1\n",
        "            else:\n",
        "                count_d[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Byd_z-CLQq5",
        "colab_type": "code",
        "outputId": "52311715-966a-4383-e3f2-5e21eab0c15d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#the size of the vocabulary\n",
        "word_counts = {}\n",
        "\n",
        "words_counter(word_counts, clean_training_tag)\n",
        "words_counter(word_counts, clean_training_plot)\n",
        "            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 37183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SRQTXw2-LQrH",
        "colab_type": "code",
        "outputId": "3902963b-229c-4a8c-e983-34b6360499d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#For Language Embedding we are using Refernce from Conceptnet Numberbatch's (CN) embeddings\n",
        "\n",
        "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
        "embeddings_index = {}\n",
        "with open('gdrive/My Drive/numberbatch-en.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))\n",
        "#embeddings_index.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 417195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NmTzhHv8LQrT",
        "colab_type": "code",
        "outputId": "a8f244ba-beff-404c-daac-39e9eaf32118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Find missing words from from conceptnet-numberbatch, and are used more than our threshold.\n",
        "missing_words = 0\n",
        "threshold = 5\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round((1.0*missing_words/len(word_counts))*100,4)\n",
        "\n",
        "print(\"Number of words missing from CN:\", missing_words)\n",
        "#print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words missing from CN: 184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZzurM03VlzJa",
        "colab_type": "code",
        "outputId": "634d1f29-7f34-4cc3-bd73-e394fc0f6a00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#converting words to integers\n",
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "#tokens that will be used to our vocab and for seqtoseq model \n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "#print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 37183\n",
            "Number of words we will use: 32311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d_c9mCQtLQrx",
        "colab_type": "code",
        "outputId": "98862d04-ee5b-4c6b-ca8a-ac3a0fe4bc0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#embedding dimensions to match conceptnet-numberbatch.\n",
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Default matrix with zeroes\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in conceptnet-numberbatch, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d_pENhlVLQr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "'''Convert words in Movie_plot to an integer.\n",
        "       If word is not in vocab_to_int, use UNK's integer.\n",
        "       Count the number of words and UNKs.\n",
        "       Adding  EOS token to the end of texts for seq2seq model'''\n",
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    \n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R6UGbTa7LQsE",
        "colab_type": "code",
        "outputId": "ea6c3ed4-776c-44cf-926a-68184be0642b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Apply convert_to_ints to clean_Movie_plot and clean_taglines\n",
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_training_tag, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_training_plot, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in Movie_plot:\", word_count)\n",
        "print(\"Total number of UNKs in Movie_plot:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in Movie_plot: 495597\n",
            "Total number of UNKs in Movie_plot: 5999\n",
            "Percent of words that are UNK: 1.21%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p9JQy1ZILQsN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#DataFrame\n",
        "def create_lengths(text):\n",
        "    \n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NtMSBO3WLQsV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "#print(\"Summaries:\")\n",
        "#print(lengths_summaries.describe())\n",
        "#print()\n",
        "#print(\"Texts:\")\n",
        "#print(lengths_texts.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f7a5lIyOLQs6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Counter for UNK appears in a sentence.'''\n",
        "def unk_counter(sentence):\n",
        "    \n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k1lki7N1LQtE",
        "colab_type": "code",
        "outputId": "965bba9a-7fc6-4b21-c673-bdf8a71f45c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "# Sorting the Movie_plot and Taglines by the length of the texts, shortest to longest\n",
        "# Limiting the length of Movie_plot and Tagline based Min-Max range.\n",
        "# Remove Taglines that include too many UNKs\n",
        "\n",
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 200\n",
        "max_summary_length = 50\n",
        "min_length = 2\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in range(min(lengths_texts.counts), max_text_length): \n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "        \n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))\n",
        "print(type(sorted_summaries))\n",
        "print(type(sorted_texts))\n",
        "print(sorted_summaries[:10])\n",
        "print(sorted_texts[:10])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10664\n",
            "10664\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "[[540, 636, 540], [67, 439, 38, 1597, 1598], [9, 128, 3, 9, 392, 3, 305], [4443, 2884, 9, 7, 9, 21, 1074], [191, 296, 35, 297, 64, 113, 67, 45, 711], [63, 3, 1591, 794, 1150], [9, 8511, 277, 9, 8511, 3760, 9, 8511, 250], [99, 31, 859, 33, 27, 3092, 384, 458, 384, 109, 708], [490, 527, 4752, 33, 17, 18, 948, 81, 709, 22, 490, 527, 9, 3697, 948, 191, 948, 490, 527, 27, 67, 108, 35, 2210, 195, 35, 147, 379, 1520, 1841, 858, 371, 35, 360, 126, 1800], [9, 30, 126, 1081, 636, 57, 160, 436, 183, 513, 82, 573, 3, 329]]\n",
            "[[12236, 798, 32309], [12236, 798, 32309], [128, 392, 305, 32309], [1519, 7991, 1295, 32309], [3684, 67, 287, 32309], [1591, 794, 1150, 32309], [1704, 8511, 1601, 32309], [16035, 1635, 1699, 1118, 32309], [360, 2387, 236, 566, 32309], [542, 1292, 1832, 573, 32309]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W6DDETeALQtM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creation of the Tagline prediction Model"
      ]
    },
    {
      "metadata": {
        "id": "c6g5-4SeLQtO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_inputs():\n",
        "    \n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7EhYpPaNLQtZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    \n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XKyj-X_LQtn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    \n",
        "    \n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7_0YbtcLQt_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    \n",
        "    \n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pq5x5WfkLQuG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    \n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpnViGejLQuN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    \n",
        "    \n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "            \n",
        "    \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Ht7T0DtLQuT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    \n",
        "    \n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2oIs-ANVLQuZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HTVBMCLqLQuf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        \n",
        "        \n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vNMW6ONdLQun",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Hyper parameters\n",
        "epochs = 30\n",
        "batch_size = 64\n",
        "rnn_size = 256\n",
        "num_layers = 4\n",
        "learning_rate = 0.002\n",
        "keep_probability = 0.80"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srcAwEmPkjEx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9FNu1oJeLQuu",
        "colab_type": "code",
        "outputId": "9ab90fc3-517f-4b9f-a20e-09e4fe476390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Build the graph\n",
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph is built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KVv3Zz19LQu6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ]
    },
    {
      "metadata": {
        "id": "mUYUT0pyLQvH",
        "colab_type": "code",
        "outputId": "eac9a242-39db-440f-b13e-10529146f767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Subset of dataset for training\n",
        "start =0\n",
        "end = start + 5000\n",
        "sorted_summaries_short = sorted_summaries[start:end]\n",
        "sorted_texts_short = sorted_texts[start:end]\n",
        "print(\"Movie_plot shortest subset:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest length:\",len(sorted_texts_short[-1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Movie_plot shortest subset: 3\n",
            "The longest length: 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "lfR2LOmXLQvQ",
        "colab_type": "code",
        "outputId": "e4ad5d21-d596-4d9b-aa85-668e1ae07f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6137
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "learning_rate_decay = 0.95\n",
        "min_learning_rate = 0.0005\n",
        "display_step = 20 \n",
        "stop_early = 0 \n",
        "stop = 3 \n",
        "per_epoch = 3 \n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] \n",
        "\n",
        "checkpoint = \"best_model.ckpt\" \n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "   \n",
        "    \n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print(\"Building the Model...\")\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts_short) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "\n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "                \n",
        "                \n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        \n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the Model...\n",
            "Epoch   1/30 Batch   20/78 - Loss:  3.395, Seconds: 5.43\n",
            "Average loss for this update: 3.195\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   1/30 Batch   40/78 - Loss:  2.357, Seconds: 8.69\n",
            "Average loss for this update: 2.41\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   1/30 Batch   60/78 - Loss:  2.510, Seconds: 5.82\n",
            "Average loss for this update: 2.502\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch   2/30 Batch   20/78 - Loss:  2.219, Seconds: 5.36\n",
            "Average loss for this update: 2.212\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   2/30 Batch   40/78 - Loss:  2.186, Seconds: 8.56\n",
            "Average loss for this update: 2.259\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch   2/30 Batch   60/78 - Loss:  2.375, Seconds: 5.93\n",
            "Average loss for this update: 2.381\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch   3/30 Batch   20/78 - Loss:  2.173, Seconds: 5.28\n",
            "Average loss for this update: 2.162\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   3/30 Batch   40/78 - Loss:  2.114, Seconds: 8.55\n",
            "Average loss for this update: 2.187\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch   3/30 Batch   60/78 - Loss:  2.305, Seconds: 5.82\n",
            "Average loss for this update: 2.295\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch   4/30 Batch   20/78 - Loss:  2.106, Seconds: 5.37\n",
            "Average loss for this update: 2.094\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   4/30 Batch   40/78 - Loss:  2.037, Seconds: 8.56\n",
            "Average loss for this update: 2.102\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch   4/30 Batch   60/78 - Loss:  2.204, Seconds: 5.92\n",
            "Average loss for this update: 2.197\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch   5/30 Batch   20/78 - Loss:  2.039, Seconds: 5.39\n",
            "Average loss for this update: 2.026\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   5/30 Batch   40/78 - Loss:  1.959, Seconds: 8.56\n",
            "Average loss for this update: 2.016\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   5/30 Batch   60/78 - Loss:  2.117, Seconds: 5.95\n",
            "Average loss for this update: 2.113\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch   6/30 Batch   20/78 - Loss:  1.984, Seconds: 5.33\n",
            "Average loss for this update: 1.974\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   6/30 Batch   40/78 - Loss:  1.908, Seconds: 8.51\n",
            "Average loss for this update: 1.95\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   6/30 Batch   60/78 - Loss:  2.038, Seconds: 6.12\n",
            "Average loss for this update: 2.045\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch   7/30 Batch   20/78 - Loss:  1.917, Seconds: 5.45\n",
            "Average loss for this update: 1.907\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   7/30 Batch   40/78 - Loss:  1.831, Seconds: 8.60\n",
            "Average loss for this update: 1.879\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   7/30 Batch   60/78 - Loss:  1.972, Seconds: 5.92\n",
            "Average loss for this update: 1.976\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch   8/30 Batch   20/78 - Loss:  1.861, Seconds: 5.41\n",
            "Average loss for this update: 1.85\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   8/30 Batch   40/78 - Loss:  1.774, Seconds: 8.57\n",
            "Average loss for this update: 1.814\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   8/30 Batch   60/78 - Loss:  1.891, Seconds: 5.92\n",
            "Average loss for this update: 1.89\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch   9/30 Batch   20/78 - Loss:  1.816, Seconds: 5.44\n",
            "Average loss for this update: 1.807\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   9/30 Batch   40/78 - Loss:  1.721, Seconds: 8.53\n",
            "Average loss for this update: 1.752\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch   9/30 Batch   60/78 - Loss:  1.832, Seconds: 5.81\n",
            "Average loss for this update: 1.837\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  10/30 Batch   20/78 - Loss:  1.759, Seconds: 5.37\n",
            "Average loss for this update: 1.753\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  10/30 Batch   40/78 - Loss:  1.676, Seconds: 8.65\n",
            "Average loss for this update: 1.703\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  10/30 Batch   60/78 - Loss:  1.775, Seconds: 5.88\n",
            "Average loss for this update: 1.776\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  11/30 Batch   20/78 - Loss:  1.711, Seconds: 5.47\n",
            "Average loss for this update: 1.704\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  11/30 Batch   40/78 - Loss:  1.620, Seconds: 8.48\n",
            "Average loss for this update: 1.637\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  11/30 Batch   60/78 - Loss:  1.694, Seconds: 5.84\n",
            "Average loss for this update: 1.702\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  12/30 Batch   20/78 - Loss:  1.658, Seconds: 5.38\n",
            "Average loss for this update: 1.65\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  12/30 Batch   40/78 - Loss:  1.559, Seconds: 8.54\n",
            "Average loss for this update: 1.569\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  12/30 Batch   60/78 - Loss:  1.630, Seconds: 5.84\n",
            "Average loss for this update: 1.644\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  13/30 Batch   20/78 - Loss:  1.606, Seconds: 5.52\n",
            "Average loss for this update: 1.599\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  13/30 Batch   40/78 - Loss:  1.512, Seconds: 8.53\n",
            "Average loss for this update: 1.515\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  13/30 Batch   60/78 - Loss:  1.565, Seconds: 5.96\n",
            "Average loss for this update: 1.587\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  14/30 Batch   20/78 - Loss:  1.560, Seconds: 5.42\n",
            "Average loss for this update: 1.551\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  14/30 Batch   40/78 - Loss:  1.458, Seconds: 8.55\n",
            "Average loss for this update: 1.46\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  14/30 Batch   60/78 - Loss:  1.508, Seconds: 5.80\n",
            "Average loss for this update: 1.524\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  15/30 Batch   20/78 - Loss:  1.502, Seconds: 5.36\n",
            "Average loss for this update: 1.493\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  15/30 Batch   40/78 - Loss:  1.399, Seconds: 8.69\n",
            "Average loss for this update: 1.393\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  15/30 Batch   60/78 - Loss:  1.439, Seconds: 5.89\n",
            "Average loss for this update: 1.459\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  16/30 Batch   20/78 - Loss:  1.444, Seconds: 5.40\n",
            "Average loss for this update: 1.435\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  16/30 Batch   40/78 - Loss:  1.350, Seconds: 8.60\n",
            "Average loss for this update: 1.333\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  16/30 Batch   60/78 - Loss:  1.371, Seconds: 5.91\n",
            "Average loss for this update: 1.389\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  17/30 Batch   20/78 - Loss:  1.388, Seconds: 5.39\n",
            "Average loss for this update: 1.378\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  17/30 Batch   40/78 - Loss:  1.295, Seconds: 8.59\n",
            "Average loss for this update: 1.276\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  17/30 Batch   60/78 - Loss:  1.306, Seconds: 5.91\n",
            "Average loss for this update: 1.327\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  18/30 Batch   20/78 - Loss:  1.342, Seconds: 5.43\n",
            "Average loss for this update: 1.335\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  18/30 Batch   40/78 - Loss:  1.248, Seconds: 8.63\n",
            "Average loss for this update: 1.224\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  18/30 Batch   60/78 - Loss:  1.245, Seconds: 5.93\n",
            "Average loss for this update: 1.271\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  19/30 Batch   20/78 - Loss:  1.291, Seconds: 5.39\n",
            "Average loss for this update: 1.285\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  19/30 Batch   40/78 - Loss:  1.197, Seconds: 8.54\n",
            "Average loss for this update: 1.172\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  19/30 Batch   60/78 - Loss:  1.189, Seconds: 5.87\n",
            "Average loss for this update: 1.208\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  20/30 Batch   20/78 - Loss:  1.254, Seconds: 5.36\n",
            "Average loss for this update: 1.245\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  20/30 Batch   40/78 - Loss:  1.160, Seconds: 8.84\n",
            "Average loss for this update: 1.125\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  20/30 Batch   60/78 - Loss:  1.120, Seconds: 6.10\n",
            "Average loss for this update: 1.14\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  21/30 Batch   20/78 - Loss:  1.210, Seconds: 5.56\n",
            "Average loss for this update: 1.203\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  21/30 Batch   40/78 - Loss:  1.129, Seconds: 8.53\n",
            "Average loss for this update: 1.094\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  21/30 Batch   60/78 - Loss:  1.075, Seconds: 5.81\n",
            "Average loss for this update: 1.08\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  22/30 Batch   20/78 - Loss:  1.169, Seconds: 5.45\n",
            "Average loss for this update: 1.16\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  22/30 Batch   40/78 - Loss:  1.067, Seconds: 8.63\n",
            "Average loss for this update: 1.029\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  22/30 Batch   60/78 - Loss:  1.018, Seconds: 5.77\n",
            "Average loss for this update: 1.032\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  23/30 Batch   20/78 - Loss:  1.134, Seconds: 5.51\n",
            "Average loss for this update: 1.126\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  23/30 Batch   40/78 - Loss:  1.038, Seconds: 8.58\n",
            "Average loss for this update: 0.99\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  23/30 Batch   60/78 - Loss:  0.960, Seconds: 5.88\n",
            "Average loss for this update: 0.968\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  24/30 Batch   20/78 - Loss:  1.081, Seconds: 5.38\n",
            "Average loss for this update: 1.073\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  24/30 Batch   40/78 - Loss:  0.987, Seconds: 8.55\n",
            "Average loss for this update: 0.94\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  24/30 Batch   60/78 - Loss:  0.917, Seconds: 5.72\n",
            "Average loss for this update: 0.919\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  25/30 Batch   20/78 - Loss:  1.034, Seconds: 5.46\n",
            "Average loss for this update: 1.026\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  25/30 Batch   40/78 - Loss:  0.938, Seconds: 8.54\n",
            "Average loss for this update: 0.89\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  25/30 Batch   60/78 - Loss:  0.857, Seconds: 5.84\n",
            "Average loss for this update: 0.857\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  26/30 Batch   20/78 - Loss:  0.986, Seconds: 5.33\n",
            "Average loss for this update: 0.976\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  26/30 Batch   40/78 - Loss:  0.882, Seconds: 8.50\n",
            "Average loss for this update: 0.839\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  26/30 Batch   60/78 - Loss:  0.808, Seconds: 5.86\n",
            "Average loss for this update: 0.805\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  27/30 Batch   20/78 - Loss:  0.942, Seconds: 5.39\n",
            "Average loss for this update: 0.929\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  27/30 Batch   40/78 - Loss:  0.826, Seconds: 8.60\n",
            "Average loss for this update: 0.789\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  27/30 Batch   60/78 - Loss:  0.761, Seconds: 5.94\n",
            "Average loss for this update: 0.763\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  28/30 Batch   20/78 - Loss:  0.909, Seconds: 5.46\n",
            "Average loss for this update: 0.896\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  28/30 Batch   40/78 - Loss:  0.785, Seconds: 8.44\n",
            "Average loss for this update: 0.743\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  28/30 Batch   60/78 - Loss:  0.719, Seconds: 5.83\n",
            "Average loss for this update: 0.707\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  29/30 Batch   20/78 - Loss:  0.861, Seconds: 5.48\n",
            "Average loss for this update: 0.849\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  29/30 Batch   40/78 - Loss:  0.751, Seconds: 8.39\n",
            "Average loss for this update: 0.711\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  29/30 Batch   60/78 - Loss:  0.679, Seconds: 5.83\n",
            "Average loss for this update: 0.674\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  30/30 Batch   20/78 - Loss:  0.825, Seconds: 5.43\n",
            "Average loss for this update: 0.813\n",
            "No Improvement.\n",
            "Building the Model...\n",
            "Epoch  30/30 Batch   40/78 - Loss:  0.706, Seconds: 8.59\n",
            "Average loss for this update: 0.664\n",
            "New Record!\n",
            "Building the Model...\n",
            "Epoch  30/30 Batch   60/78 - Loss:  0.640, Seconds: 6.12\n",
            "Average loss for this update: 0.639\n",
            "New Record!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aWzhXec4zI_d",
        "colab_type": "code",
        "outputId": "75329b03-43b9-450f-b5af-b507a73963b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(st)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-29 10:29:58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PL1zHFGTzJ4l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JIXAJHJWhR-Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Train** **data**"
      ]
    },
    {
      "metadata": {
        "id": "Ja_SGEF7o9fp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    \n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iFn-aXOphBLL",
        "colab_type": "code",
        "outputId": "82bd4f3f-6cbd-41a8-9cbc-8de605fc38a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "output_summaries = []\n",
        "checkpoint = \"./best_model.ckpt\"\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "loaded_graph = tf.Graph()\n",
        "clean_train_plot_top = clean_training_plot[0:10]\n",
        "clean_train_tag_top= clean_training_tag[0:10]\n",
        "\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    \n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    \n",
        "    for i in range(0, len(clean_train_tag_top10)):\n",
        "        text = text_to_seq(clean_train_plot_top10[i])\n",
        "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(10,20)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "        temp = \" \".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
        "        output_summaries.append(temp)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "reQ1WoTZpLQc",
        "colab_type": "code",
        "outputId": "adb44aec-01f8-49b0-95e7-02753bb64002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/63/ac/b93411318529980ab7f41e59ed64ec3ffed08ead32389e29eb78585dd55d/rouge-0.3.2-py3-none-any.whl\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zk08HFk3pXAJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "from rouge import Rouge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ka88F3x6hBHk",
        "colab_type": "code",
        "outputId": "2f79ab89-8b4c-425a-d527-d55c3ae82e30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(output_summaries, clean_train_tag_top10, avg = True)\n",
        "print(st)\n",
        "print(\"Training data scores:\",scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-29 10:29:58\n",
            "Training data scores: {'rouge-1': {'f': 0.24500500883292103, 'p': 0.32107142857142856, 'r': 0.20662679425837321}, 'rouge-2': {'f': 0.11742424051143381, 'p': 0.15194805194805192, 'r': 0.09636363636363637}, 'rouge-l': {'f': 0.2202135947193904, 'p': 0.32107142857142856, 'r': 0.20662679425837321}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "agnXM-klpzft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "5PCKuVQWhoS5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Validation** ***Data***"
      ]
    },
    {
      "metadata": {
        "id": "VXYHFHaep62c",
        "colab_type": "code",
        "outputId": "8b536770-284b-4788-c47d-41ae33509c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Datetime:\",st)\n",
        "output_summaries = []\n",
        "checkpoint = \"./best_model.ckpt\"\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "loaded_graph = tf.Graph()\n",
        "clean_val_plot_top10 = clean_training_plot[0:20]\n",
        "clean_val_tag_top10 = clean_training_tag[0:20] \n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Loading model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    \n",
        "    for i in range(0, len(clean_val_tag_top10)):\n",
        "      \n",
        "        text = text_to_seq(clean_val_plot_top10[i])\n",
        "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(10,20)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "        temp = \" \".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
        "        output_summaries.append(temp)\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datetime: 2019-04-29 10:29:58\n",
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ttgfJMbWhA_m",
        "colab_type": "code",
        "outputId": "5c423bff-1699-4a18-8194-7d26d5ed45d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "scores = rouge.get_scores(output_summaries, clean_val_tag_top10, avg = True)\n",
        "print(\"Validation Data: \",(scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Data:  {'rouge-1': {'f': 0.20753608066197904, 'p': 0.29148809523809527, 'r': 0.16827551834130783}, 'rouge-2': {'f': 0.08459447274928093, 'p': 0.11097402597402595, 'r': 0.06901515151515152}, 'rouge-l': {'f': 0.17636395692073467, 'p': 0.27898809523809526, 'r': 0.16327551834130782}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bOXnuKFkh_gh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Test Data**"
      ]
    },
    {
      "metadata": {
        "id": "Y7YF5T8xrvhQ",
        "colab_type": "code",
        "outputId": "b325b449-7ca1-43dd-8054-d058c71334f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Datetime:\",st)\n",
        "output_summaries = []\n",
        "checkpoint = \"./best_model.ckpt\"\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "loaded_graph = tf.Graph()\n",
        "clean_test_plot_top10 = clean_training_plot[0:50]\n",
        "clean_test_tag_top10 = clean_training_tag[0:50] \n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Loading  saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    \n",
        "    for i in range(0, len(clean_test_tag_top10)):\n",
        "      \n",
        "        text = text_to_seq(clean_test_plot_top10[i])\n",
        "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(10,20)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "        temp = \" \".join([int_to_vocab[i] for i in answer_logits if i != pad])\n",
        "        output_summaries.append(temp)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datetime: 2019-04-29 10:29:58\n",
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j9XN8ryfhAwR",
        "colab_type": "code",
        "outputId": "162e726d-6b88-4d80-a3e0-fb8d0c0f1e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "scores = rouge.get_scores(output_summaries, clean_test_tag_top10, avg = True)\n",
        "print(\"Test Data:\",scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Data: {'rouge-1': {'f': 0.19861789291040188, 'p': 0.2636587301587301, 'r': 0.18509125549373226}, 'rouge-2': {'f': 0.08321033759052515, 'p': 0.09755627705627705, 'r': 0.08081118881118882}, 'rouge-l': {'f': 0.16037128721672184, 'p': 0.24149206349206342, 'r': 0.17064147488218698}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JAJlpfbCr-EK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Tagline Predictions**"
      ]
    },
    {
      "metadata": {
        "id": "r5LT_99MqXaW",
        "colab_type": "code",
        "outputId": "0e5ab5d1-4289-4ee0-f8f2-e5936099fc0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#input_sentence = \"Hi Surekha.I love irobot movie and the plot is as follows.A robot may not injure a human being or, through inaction, allow a human being to come to harm\"\n",
        "#text = text_to_seq(input_sentence)\n",
        "random = np.random.randint(0,len(clean_training_plot))\n",
        "input_sentence = clean_training_plot[random]\n",
        "text = text_to_seq(clean_training_plot[random])\n",
        "\n",
        "checkpoint = \"./best_model.ckpt\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Loading saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    \n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', movie_training_data.movie_plot[random])\n",
        "print('Original summary:', movie_training_data.tagLine[random])\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
            "Original Text: Terry is a suicidal voyeur who treats a dying addict to a final binge, but Terry will only do this if he promises to kill him.\n",
            "Original summary: An oddball odyssey about voyeurism, LSD and nude bowling!\n",
            "\n",
            "Text\n",
            "  Word Ids:    [14588, 4703, 14607, 3685, 1625, 1715, 2302, 19235, 14588, 5330, 778]\n",
            "  Input Words: terry suicidal voyeur treats dying addict final binge terry promises kill\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [205, 57, 58, 438, 58, 300]\n",
            "  Response Words: nothing is no shot no game\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zfd2yXGMsQil",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Genalization of our Model**"
      ]
    },
    {
      "metadata": {
        "id": "f-mPd0tgtGsL",
        "colab_type": "code",
        "outputId": "ea8bc702-63f4-4207-da41-73061f72ca6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Create your own review or use one from the dataset\n",
        "#input_sentence = \"Robbie, is set in 1998 and centres on a little girl, Gloria, who loves her nursemaid robot, Robbie. Her mother comes to believe that robots are unsafe, however, and Robbie is returned to the factory. Gloria is heartbroken. In an effort to show her that robots are machines, not people, her parents take her to see robots being assembled at a factory. One of the assembling robots is Robbie. Gloria endangers her life running to Robbie, and Robbie rescues Gloria, persuading Gloria's mother that robots can be trusted\"\n",
        "title = \"A Revenge Story\"\n",
        "input_sentence=\"When Bhallaladeva conspires against his brother to become the king of Mahishmati, he has him killed by Katappa and imprisons his wife. Years later, his brother's son returns to avenge his father's death.\"\n",
        "text = text_to_seq(input_sentence)\n",
        "#random = np.random.randint(0,len(clean_training_plot))\n",
        "#input_sentence = clean_training_plot[random]\n",
        "#text = text_to_seq(clean_training_plot[random])\n",
        "\n",
        "checkpoint = \"./best_model.ckpt\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "   \n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Title:', title)\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
            "Original Title: A Revenge Story\n",
            "\n",
            "Text\n",
            "  Word Ids:    [32307, 16640, 1309, 779, 392, 32307, 2062, 32307, 29594, 1271, 1151, 3178, 1309, 449, 1832, 3916, 1645, 500]\n",
            "  Input Words: <UNK> conspires brother become king <UNK> killed <UNK> imprisons wife years later brother son returns avenge father death\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [9, 341, 9, 341, 9, 367]\n",
            "  Response Words: the suspense the suspense the truth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8t7ziteZuenB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zrVyUKQ9ufpQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Detecting Mistakes in predicting Taglines**"
      ]
    },
    {
      "metadata": {
        "id": "aWPzoPw0sPBH",
        "colab_type": "code",
        "outputId": "a534a151-f0a7-4df5-9d47-3fbda9479c2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "input_sentence = \"Hi This is Surekha from Hyderabad.I love irobot movie and the plot is as follows.A robot may not injure a human being or, through inaction, allow a human being to come to harm\"\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "checkpoint = \"./best_model.ckpt\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
            "\n",
            "Text\n",
            "  Word Ids:    [3851, 32307, 32307, 67, 32307, 104, 2119, 2098, 935, 857, 32307, 1173, 32307, 6289, 1173, 79, 3714]\n",
            "  Input Words: hi <UNK> <UNK> love <UNK> movie plot follows robot may <UNK> human <UNK> allow human come harm\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [9, 128, 3, 9, 21]\n",
            "  Response Words: the story of the world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CwIWAH2zsld0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Mistakes:1)Names are becoming unownk for local names and places\n",
        "                  2)Repetation of words \n",
        "                  3)Not exact prediction because restriction on Tagline length and Plot\n",
        "                  4)Vocab Mistakes because of human written data"
      ]
    },
    {
      "metadata": {
        "id": "EraOyhu-d3iK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zLXzz2-TGFtw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}